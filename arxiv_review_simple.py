"""
åŸºäºarxivçš„è®ºæ–‡æ·±åº¦æœç´¢å’Œç»¼è¿°ç”Ÿæˆç³»ç»Ÿï¼ˆç®€åŒ–ç‰ˆï¼‰
è¯¥ç³»ç»Ÿå¯ä»¥æœç´¢æŒ‡å®šä¸»é¢˜çš„è®ºæ–‡ï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–çš„ç»¼è¿°æŠ¥å‘Š
"""

import os
import re
import arxiv
from typing import List, Dict
from datetime import datetime
from langchain_openai import ChatOpenAI
# æ·»åŠ LangSmithè¿½è¸ªå¯¼å…¥
from langsmith import traceable, Client
from langchain_core.documents import Document
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
import warnings

# è¿‡æ»¤distutilsç›¸å…³çš„å¼ƒç”¨è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, message=".*distutils.*")
warnings.filterwarnings("ignore", category=DeprecationWarning, message=".*distutils.*")


class ArxivReviewGenerator:
    def __init__(self):
        """åˆå§‹åŒ–arxivç»¼è¿°ç”Ÿæˆå™¨"""
        # åˆ›å»ºè¯­è¨€æ¨¡å‹
        self.llm = ChatOpenAI(
            api_key=os.environ.get("LLM_API_KEY_QWQ", "local-qwen2.5-72b-little-brother"),
            base_url=os.environ.get("LLM_BASE_URL_QWQ", "http://10.8.50.14:8814/v1"),
            model="qwq-32b-preview",
            temperature=0
        )
        
        # åˆå§‹åŒ–MapReduceé“¾
        self.map_reduce_chain = self._create_map_reduce_chain()
    
    def _create_map_reduce_chain(self):
        """åˆ›å»ºMapReduceé“¾ç”¨äºå¤„ç†è®ºæ–‡æ‘˜è¦"""
        # Mapé˜¶æ®µæç¤ºè¯
        map_template = """ä»¥ä¸‹æ˜¯è®ºæ–‡æ‘˜è¦å†…å®¹:
{context}
è¯·æ ¹æ®è¿™äº›æ‘˜è¦å†…å®¹ï¼Œæå–æ¯ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè¦ç‚¹ï¼ŒåŒ…æ‹¬ï¼š
1. ç ”ç©¶ä¸»é¢˜å’Œç›®æ ‡
2. ä½¿ç”¨çš„æ–¹æ³•æˆ–æŠ€æœ¯
3. ä¸»è¦å‘ç°æˆ–è´¡çŒ®
4. ç ”ç©¶å±€é™æ€§ï¼ˆå¦‚æœæœ‰ï¼‰
è¯·æä¾›ç®€æ´æ˜äº†çš„æ€»ç»“ã€‚

æ‘˜è¦æ€»ç»“:"""
        map_prompt = PromptTemplate.from_template(map_template)
        map_chain = map_prompt | self.llm | StrOutputParser()

        # Reduceé˜¶æ®µæç¤ºè¯
        reduce_template = """ä»¥ä¸‹æ˜¯å¤šç¯‡è®ºæ–‡çš„æ‘˜è¦æ€»ç»“:
{context}
è¯·æ ¹æ®è¿™äº›æ‘˜è¦æ€»ç»“ï¼Œè¿›è¡Œç»¼åˆåˆ†æï¼Œè¦æ±‚ï¼š
1. æŒ‰ä¸»é¢˜æˆ–æ–¹æ³•å¯¹è®ºæ–‡è¿›è¡Œåˆ†ç±»
2. è¯†åˆ«ç ”ç©¶ä¸­çš„å…±åŒç‚¹å’Œå·®å¼‚
3. æŒ‡å‡ºç ”ç©¶è¶‹åŠ¿å’Œå‘å±•æ–¹å‘
4. åˆ†æå½“å‰ç ”ç©¶çš„ç©ºç™½ç‚¹æˆ–æŒ‘æˆ˜

ç»¼åˆåˆ†ææŠ¥å‘Š:"""
        reduce_prompt = PromptTemplate.from_template(reduce_template)
        reduce_chain = reduce_prompt | self.llm | StrOutputParser()
        
        # è¿”å›mapå’Œreduceé“¾
        return {"map_chain": map_chain, "reduce_chain": reduce_chain}
    
    @traceable
    def translate_query(self, query: str) -> str:
        """å°†ä¸­æ–‡æŸ¥è¯¢ç¿»è¯‘æˆè‹±æ–‡ï¼Œä»¥ä¾¿æ›´å¥½åœ°åœ¨arXivä¸­æœç´¢"""
        # å¦‚æœæŸ¥è¯¢å·²ç»æ˜¯è‹±æ–‡ï¼Œç›´æ¥è¿”å›
        if re.match(r'^[a-zA-Z\s]+$', query):
            return query
        
        prompt = f"""
è¯·å°†ä»¥ä¸‹ä¸­æ–‡ç¿»è¯‘æˆé€‚åˆåœ¨å­¦æœ¯æœç´¢å¼•æ“ä¸­ä½¿ç”¨çš„è‹±æ–‡æœ¯è¯­ï¼š
ä¸­æ–‡ï¼š{query}

è¦æ±‚ï¼š
1. ç¿»è¯‘æˆå‡†ç¡®çš„å­¦æœ¯è‹±æ–‡æœ¯è¯­
2. å¦‚æœæœ‰å¤šä¸ªå¯èƒ½çš„ç¿»è¯‘ï¼Œæä¾›æœ€å¸¸è§çš„é‚£ä¸ª
3. åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹
"""
        
        try:
            translation = self.llm.invoke(prompt)
            translated_query = translation.content.strip()
            print(f"ğŸ” æŸ¥è¯¢è¯ç¿»è¯‘: '{query}' -> '{translated_query}'")
            return translated_query
        except Exception as e:
            print(f"âš ï¸ ç¿»è¯‘æŸ¥è¯¢è¯æ—¶å‡ºé”™: {str(e)}, ä½¿ç”¨åŸå§‹æŸ¥è¯¢è¯")
            return query
    
    @traceable
    def expand_query(self, query: str) -> str:
        """æ‰©å±•æŸ¥è¯¢è¯ï¼Œæ·»åŠ ç›¸å…³æœ¯è¯­ä»¥æé«˜æœç´¢æ•ˆæœ"""
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹å­¦æœ¯ä¸»é¢˜æä¾›ç›¸å…³çš„è‹±æ–‡å…³é”®è¯ï¼Œç”¨äºåœ¨arXivä¸­æœç´¢ï¼š
ä¸»é¢˜ï¼š{query}

è¦æ±‚ï¼š
1. æä¾›3-5ä¸ªæœ€ç›¸å…³çš„è‹±æ–‡å…³é”®è¯æˆ–çŸ­è¯­
2. åŒ…æ‹¬è¯¥é¢†åŸŸçš„æ ‡å‡†æœ¯è¯­å’Œå¯èƒ½çš„åŒä¹‰è¯
3. ç”¨ORè¿æ¥è¿™äº›å…³é”®è¯ï¼Œå½¢æˆä¸€ä¸ªæœç´¢è¡¨è¾¾å¼
4. åªè¿”å›æœç´¢è¡¨è¾¾å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Š

ä¾‹å¦‚ï¼šmachine learning OR deep learning OR neural networks OR AI
"""
        
        try:
            expansion = self.llm.invoke(prompt)
            expanded_query = expansion.content.strip()
            print(f"ğŸ” æŸ¥è¯¢è¯æ‰©å±•: '{query}' -> '{expanded_query}'")
            return expanded_query
        except Exception as e:
            print(f"âš ï¸ æ‰©å±•æŸ¥è¯¢è¯æ—¶å‡ºé”™: {str(e)}, ä½¿ç”¨åŸå§‹æŸ¥è¯¢è¯")
            return query
    
    @traceable
    def search_papers(self, query: str, max_results: int = 5) -> List[Dict]:
        """æœç´¢arxivè®ºæ–‡"""
        # é¦–å…ˆå°è¯•ç¿»è¯‘æŸ¥è¯¢è¯
        translated_query = self.translate_query(query)
        
        # ç„¶åæ‰©å±•æŸ¥è¯¢è¯
        search_query = self.expand_query(translated_query)
        
        try:
            client = arxiv.Client()
            search = arxiv.Search(
                query=search_query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.Relevance,
                sort_order=arxiv.SortOrder.Descending
            )
            
            papers = []
            for result in client.results(search):
                paper = {
                    "title": result.title,
                    "authors": [author.name for author in result.authors],
                    "summary": result.summary,
                    "published": result.published.strftime("%Y-%m-%d"),
                    "pdf_url": result.pdf_url,
                    "entry_id": result.entry_id,
                    "categories": result.categories
                }
                papers.append(paper)
            
            return papers
        except Exception as e:
            print(f"æœç´¢è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
            return []
    
    @traceable
    def analyze_papers_with_map_reduce(self, papers: List[Dict]) -> str:
        """ä½¿ç”¨MapReduceæ–¹æ³•åˆ†æè®ºæ–‡æ‘˜è¦"""
        if not papers:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡ã€‚"
        
        # å°†è®ºæ–‡æ‘˜è¦è½¬æ¢ä¸ºDocumentå¯¹è±¡
        docs = []
        for i, paper in enumerate(papers):
            doc_content = f"""
è®ºæ–‡ {i+1}:
æ ‡é¢˜: {paper['title']}
ä½œè€…: {', '.join(paper['authors'][:3])} ç­‰
å‘è¡¨æ—¥æœŸ: {paper['published']}
åˆ†ç±»: {', '.join(paper['categories'])}
æ‘˜è¦: {paper['summary']}
            """
            docs.append(Document(page_content=doc_content))
        
        try:
            # ä½¿ç”¨MapReduceé“¾å¤„ç†æ–‡æ¡£
            map_chain = self.map_reduce_chain["map_chain"]
            reduce_chain = self.map_reduce_chain["reduce_chain"]
            
            # Mapé˜¶æ®µï¼šå¯¹æ¯ç¯‡è®ºæ–‡è¿›è¡Œåˆ†æ
            map_results = []
            for doc in docs:
                result = map_chain.invoke({"context": doc.page_content})
                map_results.append(result)
            
            # Reduceé˜¶æ®µï¼šç»¼åˆåˆ†ææ‰€æœ‰ç»“æœ
            combined_context = "\n\n".join([f"è®ºæ–‡åˆ†æ {i+1}:\n{result}" for i, result in enumerate(map_results)])
            final_result = reduce_chain.invoke({"context": combined_context})
            
            return final_result
        except Exception as e:
            return f"ä½¿ç”¨MapReduceåˆ†æè®ºæ–‡æ—¶å‡ºé”™: {str(e)}"
    
    @traceable
    def analyze_papers(self, papers: List[Dict]) -> str:
        """åˆ†æè®ºæ–‡å¹¶æå–å…³é”®ä¿¡æ¯"""
        if not papers:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡ã€‚"
        
        # ä½¿ç”¨MapReduceæ–¹æ³•åˆ†æè®ºæ–‡
        map_reduce_analysis = self.analyze_papers_with_map_reduce(papers)
        
        # æ„å»ºè®ºæ–‡ä¿¡æ¯æ€»ç»“
        paper_summaries = []
        for i, paper in enumerate(papers):
            summary = f"""
è®ºæ–‡ {i+1}:
æ ‡é¢˜: {paper['title']}
ä½œè€…: {', '.join(paper['authors'][:3])} ç­‰
å‘è¡¨æ—¥æœŸ: {paper['published']}
åˆ†ç±»: {', '.join(paper['categories'])}
æ‘˜è¦: {paper['summary'][:500]}...
            """
            paper_summaries.append(summary)
        
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹{len(papers)}ç¯‡è®ºæ–‡å¹¶æå–å…³é”®ä¿¡æ¯ï¼š

{chr(10).join(paper_summaries)}

è¯·æŒ‰ä»¥ä¸‹è¦æ±‚è¿›è¡Œåˆ†æï¼š
1. æŒ‰ç ”ç©¶ä¸»é¢˜æˆ–æ–¹æ³•å¯¹è®ºæ–‡è¿›è¡Œåˆ†ç±»
2. æå–æ¯ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®
3. è¯†åˆ«ç ”ç©¶ä¸­çš„å…±åŒç‚¹å’Œå·®å¼‚
4. æŒ‡å‡ºç ”ç©¶è¶‹åŠ¿å’Œå‘å±•æ–¹å‘
"""
        
        try:
            analysis = self.llm.invoke(prompt)
            # ç»“åˆMapReduceåˆ†æç»“æœå’ŒLLMåˆ†æç»“æœ
            combined_analysis = f"""
MapReduceåˆ†æç»“æœ:
{map_reduce_analysis}

è¯¦ç»†åˆ†æç»“æœ:
{analysis.content}
            """
            return combined_analysis
        except Exception as e:
            return f"åˆ†æè®ºæ–‡æ—¶å‡ºé”™: {str(e)}"
    
    @traceable
    def generate_review(self, topic: str, papers: List[Dict], analysis: str) -> str:
        """ç”Ÿæˆç»¼è¿°æŠ¥å‘Š"""
        if not analysis:
            return "æ²¡æœ‰åˆ†æå†…å®¹å¯ç”¨äºç”Ÿæˆç»¼è¿°ã€‚"
        
        # æ„å»ºå‚è€ƒæ–‡çŒ®åˆ—è¡¨
        references = []
        for i, paper in enumerate(papers):
            ref = f"{i+1}. {', '.join(paper['authors'][:3])} et al. {paper['title']}. arXiv:{paper['entry_id'].split('/')[-1]}. {paper['published']}."
            references.append(ref)
        
        prompt = f"""
åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œç°åœ¨è¯·ç”Ÿæˆä¸€ç¯‡å®Œæ•´çš„å­¦æœ¯ç»¼è¿°æŠ¥å‘Šã€‚

ç ”ç©¶ä¸»é¢˜: {topic}

è®ºæ–‡åˆ†æ:
{analysis}

å‚è€ƒæ–‡çŒ®:
{chr(10).join(references)}

è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç”Ÿæˆç»¼è¿°æŠ¥å‘Šï¼Œä½¿ç”¨Markdownæ ¼å¼ï¼š

# å…³äº{topic}çš„æ–‡çŒ®ç»¼è¿°

## 1. å¼•è¨€
ä»‹ç»ç ”ç©¶é¢†åŸŸå’Œä¸»é¢˜èƒŒæ™¯

## 2. ç ”ç©¶ç°çŠ¶
æŒ‰ä¸»é¢˜æˆ–æ–¹æ³•åˆ†ç±»è®¨è®ºç›¸å…³ç ”ç©¶

## 3. ä¸»è¦æ–¹æ³•
æ€»ç»“å¸¸ç”¨çš„ç ”ç©¶æ–¹æ³•å’ŒæŠ€æœ¯

## 4. æŒ‘æˆ˜ä¸æœºé‡
åˆ†æå½“å‰é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘

## 5. ç»“è®º
æ€»ç»“æ•´ä½“ç ”ç©¶çŠ¶å†µå¹¶æå‡ºå±•æœ›

## å‚è€ƒæ–‡çŒ®
{chr(10).join(references)}

è¦æ±‚ï¼š
- å†…å®¹ä¸“ä¸šã€å‡†ç¡®ã€è¿è´¯
- ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘æ€§å¼º
- å­—æ•°ä¸å°‘äº800å­—
"""
        
        try:
            review = self.llm.invoke(prompt)
            return review.content
        except Exception as e:
            return f"ç”Ÿæˆç»¼è¿°æ—¶å‡ºé”™: {str(e)}"
    
    @traceable
    def save_review(self, review_content: str, topic: str) -> str:
        """ä¿å­˜ç»¼è¿°æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if not review_content:
            return "æ²¡æœ‰å†…å®¹å¯ä¿å­˜ã€‚"
        
        # æ¸…ç†æ–‡ä»¶å
        filename = re.sub(r'[^\w\s-]', '', topic).strip().replace(' ', '_')
        if not filename:
            filename = "literature_review"
        
        # æ·»åŠ æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename}_{timestamp}.md"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(review_content)
            return filename
        except Exception as e:
            return f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"
    
    @traceable
    def generate_review_for_topic(self, topic: str, max_papers: int = 5) -> str:
        """ä¸ºæŒ‡å®šä¸»é¢˜ç”Ÿæˆç»¼è¿°æŠ¥å‘Š"""
        print(f"ğŸ” æ­£åœ¨æœç´¢å…³äº'{topic}'çš„è®ºæ–‡...")
        papers = self.search_papers(topic, max_papers)
        print(f"âœ… æ‰¾åˆ° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡")
        
        if not papers:
            return "æœªèƒ½æ‰¾åˆ°ç›¸å…³è®ºæ–‡ã€‚"
        
        print("ğŸ“Š æ­£åœ¨åˆ†æè®ºæ–‡...")
        analysis = self.analyze_papers(papers)
        print("âœ… è®ºæ–‡åˆ†æå®Œæˆ")
        
        print("ğŸ“ æ­£åœ¨ç”Ÿæˆç»¼è¿°æŠ¥å‘Š...")
        review = self.generate_review(topic, papers, analysis)
        print("âœ… ç»¼è¿°æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜æŠ¥å‘Š...")
        filename = self.save_review(review, topic)
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜è‡³: {filename}")
        
        return filename


def run_chat_mode():
    """è¿è¡Œå¯¹è¯æ¨¡å¼"""
    print("ğŸ“š åŸºäºarxivçš„è®ºæ–‡æ·±åº¦æœç´¢å’Œç»¼è¿°ç”Ÿæˆç³»ç»Ÿ (å¯¹è¯æ¨¡å¼)")
    print("=" * 60)
    print("æ”¯æŒçš„å‘½ä»¤:")
    print("  - è¾“å…¥ç ”ç©¶ä¸»é¢˜ä»¥ç”Ÿæˆæ–‡çŒ®ç»¼è¿°")
    print("  - 'help' æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
    print("  - 'quit' æˆ– 'exit' é€€å‡ºç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆå§‹åŒ–LangSmithå®¢æˆ·ç«¯
    client = Client()
    
    # åˆ›å»ºç»¼è¿°ç”Ÿæˆå™¨
    generator = ArxivReviewGenerator()
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nè¯·è¾“å…¥æ‚¨çš„ç ”ç©¶ä¸»é¢˜æˆ–å‘½ä»¤: ").strip()
            
            # å¤„ç†é€€å‡ºå‘½ä»¤
            if user_input.lower() in ["quit", "exit", "é€€å‡º"]:
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            
            # å¤„ç†å¸®åŠ©å‘½ä»¤
            if user_input.lower() in ["help", "å¸®åŠ©"]:
                print("\nğŸ“– å¸®åŠ©ä¿¡æ¯:")
                print("  1. è¾“å…¥æ‚¨æ„Ÿå…´è¶£çš„ç ”ç©¶ä¸»é¢˜ï¼ˆä¸­æ–‡æˆ–è‹±æ–‡ï¼‰")
                print("  2. ç³»ç»Ÿå°†è‡ªåŠ¨ç¿»è¯‘å¹¶æ‰©å±•æŸ¥è¯¢è¯")
                print("  3. ç³»ç»Ÿä¼šæœç´¢ç›¸å…³è®ºæ–‡å¹¶ç”Ÿæˆç»¼è¿°æŠ¥å‘Š")
                print("  4. æŠ¥å‘Šå°†ä¿å­˜ä¸ºMarkdownæ–‡ä»¶")
                print("\nğŸ“ ç¤ºä¾‹ä¸»é¢˜:")
                print("   - æœºå™¨å­¦ä¹ ")
                print("   - äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨")
                print("   - deep learning")
                continue
            
            # å¤„ç†ç©ºè¾“å…¥
            if not user_input:
                print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„ç ”ç©¶ä¸»é¢˜æˆ–å‘½ä»¤")
                continue
            
            # ç”Ÿæˆç»¼è¿°
            print(f"\nğŸš€ å¼€å§‹å¤„ç†ä¸»é¢˜: {user_input}")
            filename = generator.generate_review_for_topic(user_input)
            print(f"\nğŸ‰ æ–‡çŒ®ç»¼è¿°å·²æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜è‡³: {filename}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œç³»ç»Ÿé€€å‡ºï¼")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
            print("ğŸ”§ è¯·æ£€æŸ¥æ‚¨çš„è¾“å…¥æˆ–ç½‘ç»œè¿æ¥åé‡è¯•")


def main():
    """ä¸»å‡½æ•°"""
    run_chat_mode()


if __name__ == "__main__":
    # è®¾ç½®LangSmithç¯å¢ƒå˜é‡ï¼ˆå¦‚æœå°šæœªè®¾ç½®ï¼‰
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
    # è¯·ç¡®ä¿è®¾ç½®æ‚¨çš„LangSmith APIå¯†é’¥
    os.environ["LANGCHAIN_API_KEY"] = "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"
    
    # æ£€æŸ¥LangSmithé…ç½®
    langchain_api_key = os.environ.get("LANGCHAIN_API_KEY")
    if not langchain_api_key:
        print("âš ï¸  æ³¨æ„: æœªé…ç½®LANGCHAIN_API_KEYç¯å¢ƒå˜é‡ï¼Œå°†ä¸ä¼šè¿›è¡ŒLangSmithè¿½è¸ª")
        print("   å¦‚éœ€å¯ç”¨è¿½è¸ªï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡:")
        print("   export LANGCHAIN_API_KEY=your-api-key")
        print("   export LANGCHAIN_TRACING_V2=true")
        print("   export LANGCHAIN_ENDPOINT=https://api.smith.langchain.com")
        print("=" * 50)
    
    main()