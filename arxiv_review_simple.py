"""
åŸºäºarxivçš„è®ºæ–‡æ·±åº¦æœç´¢å’Œç»¼è¿°ç”Ÿæˆç³»ç»Ÿï¼ˆç®€åŒ–ç‰ˆï¼‰
è¯¥ç³»ç»Ÿå¯ä»¥æœç´¢æŒ‡å®šä¸»é¢˜çš„è®ºæ–‡ï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–çš„ç»¼è¿°æŠ¥å‘Š
"""

import os
import re
import arxiv
import requests
import io
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
try:
    import PyPDF2
    PDF_PROCESSING_AVAILABLE = True
except ImportError:
    PDF_PROCESSING_AVAILABLE = False
    print("è­¦å‘Š: PyPDF2æœªå®‰è£…ï¼Œå°†ä½¿ç”¨è®ºæ–‡æ‘˜è¦è€Œä¸æ˜¯å®Œæ•´å†…å®¹")
from typing import List, Dict
from datetime import datetime
from langchain_openai import ChatOpenAI
# æ·»åŠ LangSmithè¿½è¸ªå¯¼å…¥
from langsmith import traceable, Client
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import warnings

# è¿‡æ»¤distutilsç›¸å…³çš„å¼ƒç”¨è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, message=".*distutils.*")
warnings.filterwarnings("ignore", category=DeprecationWarning, message=".*distutils.*")


class ArxivReviewGenerator:
    def __init__(self):
        """åˆå§‹åŒ–arxivç»¼è¿°ç”Ÿæˆå™¨"""
        # åˆ›å»ºè¯­è¨€æ¨¡å‹
        self.llm = ChatOpenAI(
            api_key=os.environ.get("LLM_API_KEY_QWQ", "local-qwen2.5-72b-little-brother"),
            base_url=os.environ.get("LLM_BASE_URL_QWQ", "http://10.8.50.14:8814/v1"),
            model="qwq-32b-preview",
            temperature=0
        )
        
        # åˆå§‹åŒ–MapReduceé“¾
        self.map_reduce_chain = self._create_map_reduce_chain()
    
    def _create_map_reduce_chain(self):
        """åˆ›å»ºMapReduceé“¾ç”¨äºå¤„ç†è®ºæ–‡æ‘˜è¦"""
        # Mapé˜¶æ®µæç¤ºè¯
        map_template = """ä»¥ä¸‹æ˜¯è®ºæ–‡æ‘˜è¦å†…å®¹:
{context}
è¯·æ ¹æ®è¿™äº›æ‘˜è¦å†…å®¹ï¼Œæå–æ¯ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè¦ç‚¹ï¼ŒåŒ…æ‹¬ï¼š
1. ç ”ç©¶ä¸»é¢˜å’Œç›®æ ‡
2. ä½¿ç”¨çš„æ–¹æ³•æˆ–æŠ€æœ¯ ï¼ˆè¦æ±‚ï¼šè¯¦ç»†å…·ä½“ï¼Œæœ‰ç»†èŠ‚ï¼Œè¦æ·±åˆ»å…·ä½“ï¼‰
3. ä¸»è¦å‘ç°æˆ–è´¡çŒ® ï¼ˆè¦æ±‚ï¼šè¯¦ç»†å…·ä½“ï¼Œæœ‰ç»†èŠ‚ï¼Œè¦æ·±åˆ»å…·ä½“ï¼‰
4. ç ”ç©¶å±€é™æ€§ï¼ˆå¦‚æœæœ‰ï¼‰ ï¼ˆè¦æ±‚ï¼šè¯¦ç»†å…·ä½“ï¼Œæœ‰ç»†èŠ‚ï¼Œè¦æ·±åˆ»å…·ä½“ï¼‰
è¯·æä¾›åˆ›æ–°å¤„çš„ä¸“ä¸šå…·ä½“è¯¦ç»†çš„æ€»ç»“ã€‚

æ‘˜è¦æ€»ç»“:"""
        map_prompt = PromptTemplate.from_template(map_template)
        map_chain = map_prompt | self.llm | StrOutputParser()

        # Reduceé˜¶æ®µæç¤ºè¯
        reduce_template = """ä»¥ä¸‹æ˜¯å¤šç¯‡è®ºæ–‡çš„æ‘˜è¦æ€»ç»“:
{context}
è¯·æ ¹æ®è¿™äº›æ‘˜è¦æ€»ç»“ï¼Œè¿›è¡Œç»¼åˆåˆ†æï¼Œè¦æ±‚ï¼š
1. æŒ‰ä¸»é¢˜æˆ–æ–¹æ³•å¯¹è®ºæ–‡è¿›è¡Œåˆ†ç±»  ï¼ˆè¦æ±‚ï¼šè¯¦ç»†å…·ä½“ï¼Œæœ‰ç»†èŠ‚ï¼Œè¦æ·±åˆ»å…·ä½“ï¼‰
2. è¯†åˆ«ç ”ç©¶ä¸­çš„å…±åŒç‚¹å’Œå·®å¼‚ ï¼ˆè¦æ±‚ï¼šè¯¦ç»†å…·ä½“ï¼Œæœ‰ç»†èŠ‚ï¼Œè¦æ·±åˆ»å…·ä½“ï¼‰
3. æŒ‡å‡ºå…±åŒç ”ç©¶è¶‹åŠ¿ 
4. åˆ†æå½“å‰ç ”ç©¶çš„ç©ºç™½ç‚¹æˆ–æŒ‘æˆ˜

ç»¼åˆåˆ†ææŠ¥å‘Š:"""
        reduce_prompt = PromptTemplate.from_template(reduce_template)
        reduce_chain = reduce_prompt | self.llm | StrOutputParser()
        
        # è¿”å›mapå’Œreduceé“¾
        return {"map_chain": map_chain, "reduce_chain": reduce_chain}
    
    @traceable
    def translate_query(self, query: str) -> str:
        """å°†ä¸­æ–‡æŸ¥è¯¢ç¿»è¯‘æˆè‹±æ–‡ï¼Œä»¥ä¾¿æ›´å¥½åœ°åœ¨arXivä¸­æœç´¢"""
        # å¦‚æœæŸ¥è¯¢å·²ç»æ˜¯è‹±æ–‡ï¼Œç›´æ¥è¿”å›
        if re.match(r'^[a-zA-Z\s]+$', query):
            print("ğŸ“ è¾“å…¥æŸ¥è¯¢è¯å·²ç»æ˜¯è‹±æ–‡ï¼Œæ— éœ€ç¿»è¯‘")
            return query
        
        prompt = f"""
è¯·å°†ä»¥ä¸‹ä¸­æ–‡ç¿»è¯‘æˆé€‚åˆåœ¨å­¦æœ¯æœç´¢å¼•æ“ä¸­ä½¿ç”¨çš„è‹±æ–‡æœ¯è¯­ï¼š
ä¸­æ–‡ï¼š{query}

è¦æ±‚ï¼š
1. ç¿»è¯‘æˆå‡†ç¡®çš„å­¦æœ¯è‹±æ–‡æœ¯è¯­
2. å¦‚æœæœ‰å¤šä¸ªå¯èƒ½çš„ç¿»è¯‘ï¼Œæä¾›æœ€å¸¸è§çš„é‚£ä¸ª
3. åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹
"""
        
        try:
            print(f"ğŸ¤– æ­£åœ¨è°ƒç”¨LLMè¿›è¡ŒæŸ¥è¯¢è¯ç¿»è¯‘: '{query}'")
            start_time = time.time()
            translation = self.llm.invoke(prompt)
            end_time = time.time()
            translated_query = translation.content.strip()
            print(f"âœ… æŸ¥è¯¢è¯ç¿»è¯‘å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
            print(f"ğŸ” æŸ¥è¯¢è¯ç¿»è¯‘: '{query}' -> '{translated_query}'")
            return translated_query
        except Exception as e:
            print(f"âš ï¸ ç¿»è¯‘æŸ¥è¯¢è¯æ—¶å‡ºé”™: {str(e)}, ä½¿ç”¨åŸå§‹æŸ¥è¯¢è¯")
            return query
    
    @traceable
    def expand_query(self, query: str) -> str:
        """æ‰©å±•æŸ¥è¯¢è¯ï¼Œæ·»åŠ ç›¸å…³æœ¯è¯­ä»¥æé«˜æœç´¢æ•ˆæœ"""
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹å­¦æœ¯ä¸»é¢˜æä¾›ç›¸å…³çš„è‹±æ–‡å…³é”®è¯ï¼Œç”¨äºåœ¨arXivä¸­æœç´¢ï¼š
ä¸»é¢˜ï¼š{query}

è¦æ±‚ï¼š
1. æä¾› 5ä¸ªæœ€ç›¸å…³çš„è‹±æ–‡å…³é”®è¯æˆ–çŸ­è¯­
2. åŒ…æ‹¬è¯¥é¢†åŸŸçš„æ ‡å‡†æœ¯è¯­å’Œå¯èƒ½çš„åŒä¹‰è¯
3. ç”¨ORè¿æ¥è¿™äº›å…³é”®è¯ï¼Œå½¢æˆä¸€ä¸ªæœç´¢è¡¨è¾¾å¼
4. åªè¿”å›æœç´¢è¡¨è¾¾å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Š

ä¾‹å¦‚ï¼šmachine learning OR deep learning OR neural networks OR AI
"""
        
        try:
            print(f"ğŸ¤– æ­£åœ¨è°ƒç”¨LLMè¿›è¡ŒæŸ¥è¯¢è¯æ‰©å±•: '{query}'")
            start_time = time.time()
            expansion = self.llm.invoke(prompt)
            end_time = time.time()
            expanded_query = expansion.content.strip()
            print(f"âœ… æŸ¥è¯¢è¯æ‰©å±•å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
            print(f"ğŸ” æŸ¥è¯¢è¯æ‰©å±•: '{query}' -> '{expanded_query}'")
            return expanded_query
        except Exception as e:
            print(f"âš ï¸ æ‰©å±•æŸ¥è¯¢è¯æ—¶å‡ºé”™: {str(e)}, ä½¿ç”¨åŸå§‹æŸ¥è¯¢è¯")
            return query
    
    @traceable
    def search_papers(self, query: str, max_results: int = 5) -> List[Dict]:
        """æœç´¢arxivè®ºæ–‡"""
        print(f"ğŸ” å¼€å§‹æœç´¢è®ºæ–‡ï¼ŒæŸ¥è¯¢è¯: '{query}'")
        # é¦–å…ˆå°è¯•ç¿»è¯‘æŸ¥è¯¢è¯
        print("ğŸ”„ æ­¥éª¤1: ç¿»è¯‘æŸ¥è¯¢è¯")
        translated_query = self.translate_query(query)
        
        # ç„¶åæ‰©å±•æŸ¥è¯¢è¯
        print("ğŸ”„ æ­¥éª¤2: æ‰©å±•æŸ¥è¯¢è¯")
        search_query = self.expand_query(translated_query)
        
        try:
            print(f"ğŸ” æ­£åœ¨ä½¿ç”¨æŸ¥è¯¢è¯æœç´¢è®ºæ–‡: '{search_query}'")
            client = arxiv.Client()
            search = arxiv.Search(
                query=search_query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.Relevance,
                sort_order=arxiv.SortOrder.Descending
            )
            
            papers = []
            for i, result in enumerate(client.results(search)):
                print(f"ğŸ“„ è·å–åˆ°è®ºæ–‡ {i+1}: {result.title}")
                paper = {
                    "title": result.title,
                    "authors": [author.name for author in result.authors],
                    "summary": result.summary,
                    "published": result.published.strftime("%Y-%m-%d"),
                    "pdf_url": result.pdf_url,
                    "entry_id": result.entry_id,
                    "categories": result.categories
                }
                
                # å°è¯•æå–PDFå†…å®¹
                if PDF_PROCESSING_AVAILABLE:
                    try:
                        print(f"ğŸ“„ æ­£åœ¨æå–è®ºæ–‡PDFå†…å®¹: {result.title}")
                        paper["full_content"] = self._extract_pdf_content(result.pdf_url)
                        print(f"âœ… è®ºæ–‡PDFå†…å®¹æå–å®Œæˆ: {result.title}")
                    except Exception as e:
                        print(f"âš ï¸ æå–PDFå†…å®¹æ—¶å‡ºé”™: {str(e)}, ä½¿ç”¨æ‘˜è¦å†…å®¹")
                        paper["full_content"] = result.summary
                else:
                    paper["full_content"] = result.summary
                    print(f"ğŸ“„ ä½¿ç”¨è®ºæ–‡æ‘˜è¦å†…å®¹: {result.title}")
                    
                papers.append(paper)
            
            print(f"âœ… è®ºæ–‡æœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            return papers
        except Exception as e:
            print(f"æœç´¢è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
            return []
    
    @traceable
    def _extract_pdf_content(self, pdf_url: str) -> str:
        """ä»PDFé“¾æ¥æå–å†…å®¹"""
        try:
            print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½PDFæ–‡ä»¶: {pdf_url}")
            # ä¸‹è½½PDFæ–‡ä»¶
            response = requests.get(pdf_url, timeout=30)
            response.raise_for_status()
            print("âœ… PDFæ–‡ä»¶ä¸‹è½½å®Œæˆ")
            
            # ä½¿ç”¨PyPDF2è¯»å–PDFå†…å®¹
            print("ğŸ“„ æ­£åœ¨è§£æPDFå†…å®¹")
            pdf_file = io.BytesIO(response.content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            # æå–å‰10é¡µçš„å†…å®¹ï¼ˆé€šå¸¸æ˜¯æ‘˜è¦ã€å¼•è¨€å’Œæ–¹æ³•éƒ¨åˆ†ï¼‰
            content = ""
            pages_to_extract = min(20, len(pdf_reader.pages))
            print(f"ğŸ“„ æ­£åœ¨æå–PDFå‰ {pages_to_extract} é¡µå†…å®¹")
            for i in range(pages_to_extract):
                page = pdf_reader.pages[i]
                content += page.extract_text() + "\n"
            
            print("âœ… PDFå†…å®¹æå–å®Œæˆ")
            return content
        except Exception as e:
            raise Exception(f"æå–PDFå†…å®¹å¤±è´¥: {str(e)}")
    
    @traceable
    def _extract_core_content(self, full_content: str) -> str:
        """ä»è®ºæ–‡å®Œæ•´å†…å®¹ä¸­æå–æ ¸å¿ƒåˆ›æ–°æ”¹è¿›æ–¹æ³•å’Œå®éªŒæ•°æ®éƒ¨åˆ†"""
        try:
            # ä½¿ç”¨LLMæå–æ ¸å¿ƒå†…å®¹
            prompt = f"""
è¯·ä»ä»¥ä¸‹è®ºæ–‡å†…å®¹ä¸­æå–æ ¸å¿ƒåˆ›æ–°æ”¹è¿›æ–¹æ³•å’Œå®éªŒæ•°æ®éƒ¨åˆ†ï¼š

è®ºæ–‡å†…å®¹:
{full_content[:80000]}  # é™åˆ¶é•¿åº¦ä»¥é¿å…è¶…å‡ºä¸Šä¸‹æ–‡çª—å£

è¯·æå–ä»¥ä¸‹å†…å®¹ï¼š
1. è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æˆ–æ”¹è¿›æ–¹æ³•
2. å…³é”®çš„å®éªŒæ•°æ®å’Œç»“æœ
3. é‡è¦çš„æ€§èƒ½æŒ‡æ ‡å’Œå¯¹æ¯”ç»“æœ

è¦æ±‚ï¼š
- ä¿æŒåŸæ–‡çš„æŠ€æœ¯ç»†èŠ‚å’Œæ•°æ®å‡†ç¡®æ€§
- åªæå–æœ€å…³é”®å’Œæœ€æœ‰ä»·å€¼çš„ä¿¡æ¯ 
- ä¿æŒç®€æ´ï¼Œæ€»é•¿åº¦ä¸è¶…è¿‡80000å­—
- ä»¥ç»“æ„åŒ–çš„æ–¹å¼å‘ˆç°æå–çš„å†…å®¹

æå–ç»“æœ:
"""
            
            print("ğŸ¤– æ­£åœ¨è°ƒç”¨LLMæå–è®ºæ–‡æ ¸å¿ƒå†…å®¹")
            start_time = time.time()
            extraction = self.llm.invoke(prompt)
            end_time = time.time()
            print(f"âœ… æ ¸å¿ƒå†…å®¹æå–å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
            return extraction.content.strip()
        except Exception as e:
            print(f"æå–æ ¸å¿ƒå†…å®¹æ—¶å‡ºé”™: {str(e)}, è¿”å›å®Œæ•´å†…å®¹çš„å‰2000ä¸ªå­—ç¬¦")
            return full_content[:2000]
    
    @traceable
    def analyze_papers_with_map_reduce(self, papers: List[Dict]) -> str:
        """ä½¿ç”¨MapReduceæ–¹æ³•åˆ†æè®ºæ–‡æ‘˜è¦"""
        if not papers:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡ã€‚"
        
        print(f"ğŸ”„ å¼€å§‹ä½¿ç”¨MapReduceæ–¹æ³•åˆ†æ {len(papers)} ç¯‡è®ºæ–‡")
        # å°†è®ºæ–‡æ‘˜è¦è½¬æ¢ä¸ºDocumentå¯¹è±¡
        docs = []
        for i, paper in enumerate(papers):
            # æå–æ ¸å¿ƒå†…å®¹æ›¿ä»£æ‘˜è¦
            print(f"ğŸ“„ å¤„ç†è®ºæ–‡ {i+1}/{len(papers)}: {paper['title']}")
            core_content = self._extract_core_content(paper.get("full_content", paper["summary"]))
            
            doc_content = f"""
è®ºæ–‡ {i+1}:
æ ‡é¢˜: {paper['title']}
ä½œè€…: {', '.join(paper['authors'][:3])} ç­‰
å‘è¡¨æ—¥æœŸ: {paper['published']}
åˆ†ç±»: {', '.join(paper['categories'])}
æ ¸å¿ƒå†…å®¹: {core_content}
            """
            docs.append(Document(page_content=doc_content))
        
        try:
            # ä½¿ç”¨MapReduceé“¾å¤„ç†æ–‡æ¡£
            map_chain = self.map_reduce_chain["map_chain"]
            reduce_chain = self.map_reduce_chain["reduce_chain"]
            
            # Mapé˜¶æ®µï¼šå¯¹æ¯ç¯‡è®ºæ–‡è¿›è¡Œåˆ†æ
            print("ğŸ§  å¼€å§‹Mapé˜¶æ®µï¼šé€ç¯‡åˆ†æè®ºæ–‡")
            map_results = []
            for i, doc in enumerate(docs):
                print(f"ğŸ¤– æ­£åœ¨è°ƒç”¨LLMåˆ†æç¬¬ {i+1} ç¯‡è®ºæ–‡")
                start_time = time.time()
                result = map_chain.invoke({"context": doc.page_content})
                end_time = time.time()
                print(f"âœ… ç¬¬ {i+1} ç¯‡è®ºæ–‡åˆ†æå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
                map_results.append(result)
            
            # Reduceé˜¶æ®µï¼šç»¼åˆåˆ†ææ‰€æœ‰ç»“æœ
            print("ğŸ§  å¼€å§‹Reduceé˜¶æ®µï¼šç»¼åˆåˆ†ææ‰€æœ‰è®ºæ–‡")
            combined_context = "\n\n".join([f"è®ºæ–‡åˆ†æ {i+1}:\n{result}" for i, result in enumerate(map_results)])
            print("ğŸ¤– æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œç»¼åˆåˆ†æ")
            start_time = time.time()
            final_result = reduce_chain.invoke({"context": combined_context})
            end_time = time.time()
            print(f"âœ… ç»¼åˆåˆ†æå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
            
            print("âœ… MapReduceåˆ†æå®Œæˆ")
            return final_result
        except Exception as e:
            return f"ä½¿ç”¨MapReduceåˆ†æè®ºæ–‡æ—¶å‡ºé”™: {str(e)}"
    
    @traceable
    def analyze_papers(self, papers: List[Dict]) -> str:
        """åˆ†æè®ºæ–‡å¹¶æå–å…³é”®ä¿¡æ¯"""
        if not papers:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡ã€‚"
        
        print("ğŸ“Š å¼€å§‹åˆ†æè®ºæ–‡å†…å®¹")
        # ä½¿ç”¨MapReduceæ–¹æ³•åˆ†æè®ºæ–‡
        print("ğŸ”„ æ­¥éª¤1: ä½¿ç”¨MapReduceæ–¹æ³•åˆ†æè®ºæ–‡")
        map_reduce_analysis = self.analyze_papers_with_map_reduce(papers)
        
        # æ„å»ºè®ºæ–‡ä¿¡æ¯æ€»ç»“
        print("ğŸ”„ æ­¥éª¤2: æ„å»ºè®ºæ–‡è¯¦ç»†ä¿¡æ¯")
        paper_summaries = []
        for i, paper in enumerate(papers):
            # æå–æ ¸å¿ƒå†…å®¹æ›¿ä»£æ‘˜è¦
            core_content = self._extract_core_content(paper.get("full_content", paper["summary"]))
            
            summary = f"""
è®ºæ–‡ {i+1}:
æ ‡é¢˜: {paper['title']}
ä½œè€…: {', '.join(paper['authors'][:3])} ç­‰
å‘è¡¨æ—¥æœŸ: {paper['published']}
åˆ†ç±»: {', '.join(paper['categories'])}
æ ¸å¿ƒå†…å®¹: {core_content[:500]}...
            """
            paper_summaries.append(summary)
        
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹{len(papers)}ç¯‡è®ºæ–‡å¹¶æå–å…³é”®ä¿¡æ¯ï¼š

{chr(10).join(paper_summaries)}

è¯·æŒ‰ä»¥ä¸‹è¦æ±‚è¿›è¡Œåˆ†æï¼š
1. æŒ‰ç ”ç©¶ä¸»é¢˜æˆ–æ–¹æ³•å¯¹è®ºæ–‡è¿›è¡Œåˆ†ç±»  ï¼ˆè¦æ±‚ï¼šè¯¦ç»†å…·ä½“ï¼Œæœ‰ç»†èŠ‚ï¼Œè¦æ·±åˆ»å…·ä½“ï¼‰
2. æå–æ¯ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®  ï¼ˆè¦æ±‚ï¼šè¯¦ç»†å…·ä½“ï¼Œæœ‰ç»†èŠ‚ï¼Œè¦æ·±åˆ»å…·ä½“ï¼‰
3. è¯†åˆ«ç ”ç©¶ä¸­çš„å…±åŒç‚¹å’Œå·®å¼‚  ï¼ˆè¦æ±‚ï¼šè¯¦ç»†å…·ä½“ï¼Œæœ‰ç»†èŠ‚ï¼Œè¦æ·±åˆ»å…·ä½“ï¼‰
4. æŒ‡å‡ºç ”ç©¶è¶‹åŠ¿å’Œå‘å±•æ–¹å‘ 
"""
        
        try:
            print("ğŸ¤– æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œè¯¦ç»†è®ºæ–‡åˆ†æ")
            start_time = time.time()
            analysis = self.llm.invoke(prompt)
            end_time = time.time()
            print(f"âœ… è¯¦ç»†è®ºæ–‡åˆ†æå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
            # ç»“åˆMapReduceåˆ†æç»“æœå’ŒLLMåˆ†æç»“æœ
            combined_analysis = f"""
MapReduceåˆ†æç»“æœ:
{map_reduce_analysis}

è¯¦ç»†åˆ†æç»“æœ:
{analysis.content}
            """
            print("âœ… è®ºæ–‡åˆ†æå®Œæˆ")
            return combined_analysis
        except Exception as e:
            return f"åˆ†æè®ºæ–‡æ—¶å‡ºé”™: {str(e)}"
    
    @traceable
    def generate_review(self, topic: str, papers: List[Dict], analysis: str) -> str:
        """ç”Ÿæˆç»¼è¿°æŠ¥å‘Š"""
        if not analysis:
            return "æ²¡æœ‰åˆ†æå†…å®¹å¯ç”¨äºç”Ÿæˆç»¼è¿°ã€‚"
        
        print(f"ğŸ“ å¼€å§‹ç”Ÿæˆå…³äº'{topic}'çš„ç»¼è¿°æŠ¥å‘Š")
        # æ„å»ºå‚è€ƒæ–‡çŒ®åˆ—è¡¨
        references = []
        for i, paper in enumerate(papers):
            ref = f"{i+1}. {', '.join(paper['authors'][:3])} et al. {paper['title']}. arXiv:{paper['entry_id'].split('/')[-1]}. {paper['published']}."
            references.append(ref)
        
        prompt = f"""
åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œç°åœ¨è¯·ç”Ÿæˆä¸€ç¯‡å®Œæ•´çš„å­¦æœ¯ç»¼è¿°æŠ¥å‘Šã€‚

ç ”ç©¶ä¸»é¢˜: {topic}

è®ºæ–‡åˆ†æ:
{analysis}

å‚è€ƒæ–‡çŒ®:
{chr(10).join(references)}

è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç”Ÿæˆç»¼è¿°æŠ¥å‘Šï¼Œä½¿ç”¨Markdownæ ¼å¼ï¼š

# å…³äº{topic}çš„æ–‡çŒ®ç»¼è¿°

## 1. å¼•è¨€
ä»‹ç»ç ”ç©¶é¢†åŸŸå’Œä¸»é¢˜èƒŒæ™¯

## 2. ç ”ç©¶ç°çŠ¶
ä¸“ä¸šæŒ‰ä¸»é¢˜æˆ–æ–¹æ³•åˆ†ç±»è®¨è®ºç›¸å…³ç ”ç©¶ï¼Œå¹¶ç»™å‡ºå…³æ³¨é‡ç‚¹å’Œæ‹“å±•æ–¹å‘

## 3. ä¸»è¦æ–¹æ³•
ä¸“ä¸šæ€»ç»“ç ”ç©¶æ–¹æ³•å’ŒæŠ€æœ¯ï¼Œç»™å‡ºä¸“ä¸šçŠ€åˆ©çš„ç‚¹è¯„å’Œæ ¸å¿ƒæŠ€æœ¯åŸç†

## 4. å®ç”¨æŠ€æœ¯ã€æ¡†æ¶ç»“è®º
æ€»ç»“æ•´ä½“ç ”ç©¶çŠ¶å†µå¹¶æå‡ºå®ç”¨æŠ€æœ¯ã€æ¡†æ¶çš„ä¸“ä¸šè§‚ç‚¹

## å‚è€ƒæ–‡çŒ®
{chr(10).join(references)}

è¦æ±‚ï¼š
- å†…å®¹ä¸“ä¸šã€å‡†ç¡®ã€è¿è´¯
- ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘æ€§å¼º
- å­—æ•°ä¸å°‘äº5000å­—
"""
        
        try:
            print("ğŸ¤– æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆç»¼è¿°æŠ¥å‘Š")
            start_time = time.time()
            review = self.llm.invoke(prompt)
            end_time = time.time()
            print(f"âœ… ç»¼è¿°æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
            return review.content
        except Exception as e:
            return f"ç”Ÿæˆç»¼è¿°æ—¶å‡ºé”™: {str(e)}"
    
    @traceable
    def save_review(self, review_content: str, topic: str) -> str:
        """ä¿å­˜ç»¼è¿°æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if not review_content:
            return "æ²¡æœ‰å†…å®¹å¯ä¿å­˜ã€‚"
        
        # æ¸…ç†æ–‡ä»¶å
        filename = re.sub(r'[^\w\s-]', '', topic).strip().replace(' ', '_')
        if not filename:
            filename = "literature_review"
        
        # æ·»åŠ æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename}_{timestamp}.md"
        
        try:
            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ç»¼è¿°æŠ¥å‘Šåˆ°æ–‡ä»¶: {filename}")
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(review_content)
            print(f"âœ… ç»¼è¿°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
            return filename
        except Exception as e:
            return f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"
    
    @traceable
    def generate_review_for_topic(self, topic: str, max_papers: int = 5) -> str:
        """ä¸ºæŒ‡å®šä¸»é¢˜ç”Ÿæˆç»¼è¿°æŠ¥å‘Š"""
        print(f"ğŸ” æ­£åœ¨æœç´¢å…³äº'{topic}'çš„è®ºæ–‡...")
        papers = self.search_papers(topic, max_papers)
        print(f"âœ… æ‰¾åˆ° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡")
        
        if not papers:
            return "æœªèƒ½æ‰¾åˆ°ç›¸å…³è®ºæ–‡ã€‚"
        
        print("ğŸ“Š æ­£åœ¨åˆ†æè®ºæ–‡...")
        analysis = self.analyze_papers(papers)
        print("âœ… è®ºæ–‡åˆ†æå®Œæˆ")
        
        print("ğŸ“ æ­£åœ¨ç”Ÿæˆç»¼è¿°æŠ¥å‘Š...")
        review = self.generate_review(topic, papers, analysis)
        print("âœ… ç»¼è¿°æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜æŠ¥å‘Š...")
        filename = self.save_review(review, topic)
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜è‡³: {filename}")
        
        return filename


def run_chat_mode():
    """è¿è¡Œå¯¹è¯æ¨¡å¼"""
    print("ğŸ“š åŸºäºarxivçš„è®ºæ–‡æ·±åº¦æœç´¢å’Œç»¼è¿°ç”Ÿæˆç³»ç»Ÿ (å¯¹è¯æ¨¡å¼)")
    print("=" * 60)
    print("æ”¯æŒçš„å‘½ä»¤:")
    print("  - è¾“å…¥ç ”ç©¶ä¸»é¢˜ä»¥ç”Ÿæˆæ–‡çŒ®ç»¼è¿°")
    print("  - 'help' æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
    print("  - 'quit' æˆ– 'exit' é€€å‡ºç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆå§‹åŒ–LangSmithå®¢æˆ·ç«¯
    client = Client()
    
    # åˆ›å»ºç»¼è¿°ç”Ÿæˆå™¨
    generator = ArxivReviewGenerator()
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nè¯·è¾“å…¥æ‚¨çš„ç ”ç©¶ä¸»é¢˜æˆ–å‘½ä»¤: ").strip()
            
            # å¤„ç†é€€å‡ºå‘½ä»¤
            if user_input.lower() in ["quit", "exit", "é€€å‡º"]:
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            
            # å¤„ç†å¸®åŠ©å‘½ä»¤
            if user_input.lower() in ["help", "å¸®åŠ©"]:
                print("\nğŸ“– å¸®åŠ©ä¿¡æ¯:")
                print("  1. è¾“å…¥æ‚¨æ„Ÿå…´è¶£çš„ç ”ç©¶ä¸»é¢˜ï¼ˆä¸­æ–‡æˆ–è‹±æ–‡ï¼‰")
                print("  2. ç³»ç»Ÿå°†è‡ªåŠ¨ç¿»è¯‘å¹¶æ‰©å±•æŸ¥è¯¢è¯")
                print("  3. ç³»ç»Ÿä¼šæœç´¢ç›¸å…³è®ºæ–‡å¹¶ç”Ÿæˆç»¼è¿°æŠ¥å‘Š")
                print("  4. æŠ¥å‘Šå°†ä¿å­˜ä¸ºMarkdownæ–‡ä»¶")
                print("\nğŸ“ ç¤ºä¾‹ä¸»é¢˜:")
                print("   - æœºå™¨å­¦ä¹ ")
                print("   - äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨")
                print("   - deep learning")
                continue
            
            # å¤„ç†ç©ºè¾“å…¥
            if not user_input:
                print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„ç ”ç©¶ä¸»é¢˜æˆ–å‘½ä»¤")
                continue
            
            # ç”Ÿæˆç»¼è¿°
            print(f"\nğŸš€ å¼€å§‹å¤„ç†ä¸»é¢˜: {user_input}")
            filename = generator.generate_review_for_topic(user_input)
            print(f"\nğŸ‰ æ–‡çŒ®ç»¼è¿°å·²æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜è‡³: {filename}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œç³»ç»Ÿé€€å‡ºï¼")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
            print("ğŸ”§ è¯·æ£€æŸ¥æ‚¨çš„è¾“å…¥æˆ–ç½‘ç»œè¿æ¥åé‡è¯•")


def main():
    """ä¸»å‡½æ•°"""
    run_chat_mode()


if __name__ == "__main__":
    # è®¾ç½®LangSmithç¯å¢ƒå˜é‡ï¼ˆå¦‚æœå°šæœªè®¾ç½®ï¼‰
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
    # è¯·ç¡®ä¿è®¾ç½®æ‚¨çš„LangSmith APIå¯†é’¥
    os.environ["LANGCHAIN_API_KEY"] = "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"
    
    # æ£€æŸ¥LangSmithé…ç½®
    langchain_api_key = os.environ.get("LANGCHAIN_API_KEY")
    if not langchain_api_key:
        print("âš ï¸  æ³¨æ„: æœªé…ç½®LANGCHAIN_API_KEYç¯å¢ƒå˜é‡ï¼Œå°†ä¸ä¼šè¿›è¡ŒLangSmithè¿½è¸ª")
        print("   å¦‚éœ€å¯ç”¨è¿½è¸ªï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡:")
        print("   export LANGCHAIN_API_KEY=your-api-key")
        print("   export LANGCHAIN_TRACING_V2=true")
        print("   export LANGCHAIN_ENDPOINT=https://api.smith.langchain.com")
        print("=" * 50)
    
    main()