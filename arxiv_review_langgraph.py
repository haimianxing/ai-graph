"""
åŸºäºarxivçš„è®ºæ–‡æ·±åº¦æœç´¢å’Œç»¼è¿°ç”Ÿæˆç³»ç»Ÿï¼ˆLangGraphç‰ˆæœ¬ï¼‰
è¯¥ç³»ç»Ÿä½¿ç”¨LangGraphçš„çŠ¶æ€å›¾å®ç°å·¥ä½œæµï¼Œå¹¶å¯è§†åŒ–æ˜¾ç¤ºçŠ¶æ€å›¾
"""

import os
import re
import arxiv
import requests
import io
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
try:
    import PyPDF2
    PDF_PROCESSING_AVAILABLE = True
except ImportError:
    PDF_PROCESSING_AVAILABLE = False
    print("è­¦å‘Š: PyPDF2æœªå®‰è£…ï¼Œå°†ä½¿ç”¨è®ºæ–‡æ‘˜è¦è€Œä¸æ˜¯å®Œæ•´å†…å®¹")
from typing import List, Dict, Annotated, TypedDict, Optional
from datetime import datetime, timedelta
from langchain_openai import ChatOpenAI
# æ·»åŠ LangSmithè¿½è¸ªå¯¼å…¥
from langsmith import traceable, Client
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
# æ·»åŠ LangGraphç›¸å…³å¯¼å…¥
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from langchain_core.messages import HumanMessage, AIMessage
import warnings

# æ·»åŠ å›¾åƒæ˜¾ç¤ºç›¸å…³å¯¼å…¥
from IPython.display import Image, display

max_papers = 20

# è¿‡æ»¤distutilsç›¸å…³çš„å¼ƒç”¨è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, message=".*distutils.*")
warnings.filterwarnings("ignore", category=DeprecationWarning, message=".*distutils.*")


# å®šä¹‰çŠ¶æ€æ¨¡å‹
class GraphState(TypedDict):
    """LangGraphçŠ¶æ€æ¨¡å‹"""
    topic: str  # ç”¨æˆ·è¾“å…¥çš„ç ”ç©¶ä¸»é¢˜
    query: str  # ç¿»è¯‘å’Œæ‰©å±•åçš„æŸ¥è¯¢è¯
    papers: List[Dict]  # æœç´¢åˆ°çš„è®ºæ–‡åˆ—è¡¨
    analysis: str  # è®ºæ–‡åˆ†æç»“æœ
    review: str  # ç”Ÿæˆçš„ç»¼è¿°æŠ¥å‘Š
    filename: str  # ä¿å­˜çš„æ–‡ä»¶å
    messages: Annotated[list, add_messages]  # æ¶ˆæ¯å†å²


class ArxivReviewGenerator:
    def __init__(self):
        """åˆå§‹åŒ–arxivç»¼è¿°ç”Ÿæˆå™¨"""
        # åˆ›å»ºè¯­è¨€æ¨¡å‹
        self.llm = ChatOpenAI(
            api_key=os.environ.get("LLM_API_KEY_QWQ", "local-qwen2.5-72b-little-brother"),
            base_url=os.environ.get("LLM_BASE_URL_QWQ", "http://10.8.50.14:8814/v1"),
            model="qwq-32b-preview",
            temperature=0
        )

        # åˆå§‹åŒ–MapReduceé“¾
        self.map_reduce_chain = self._create_map_reduce_chain()
        
        # åˆ›å»ºå·¥ä½œæµå›¾
        self.workflow = self._create_workflow()
        self.app = self.workflow.compile()

    def _create_map_reduce_chain(self):
        """åˆ›å»ºMapReduceé“¾ç”¨äºå¤„ç†è®ºæ–‡æ‘˜è¦"""
        # Mapé˜¶æ®µæç¤ºè¯
        map_template = """ä»¥ä¸‹æ˜¯è®ºæ–‡æ‘˜è¦å†…å®¹:
{context}
è¯·æ ¹æ®è¿™äº›æ‘˜è¦å†…å®¹ï¼Œæå–æ¯ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè¦ç‚¹ï¼ŒåŒ…æ‹¬ï¼š
1. ç ”ç©¶ä¸»é¢˜å’Œç›®æ ‡
2. ä½¿ç”¨çš„æ–¹æ³•æˆ–æŠ€æœ¯ ï¼ˆè¦æ±‚ï¼šè¯¦ç»†å…·ä½“ï¼Œæœ‰ç»†èŠ‚ï¼Œè¦æ·±åˆ»å…·ä½“ï¼‰
3. ä¸»è¦å‘ç°æˆ–è´¡çŒ® ï¼ˆè¦æ±‚ï¼šè¯¦ç»†å…·ä½“ï¼Œæœ‰ç»†èŠ‚ï¼Œè¦æ·±åˆ»å…·ä½“ï¼‰
4. ç ”ç©¶å±€é™æ€§ï¼ˆå¦‚æœæœ‰ï¼‰ ï¼ˆè¦æ±‚ï¼šè¯¦ç»†å…·ä½“ï¼Œæœ‰ç»†èŠ‚ï¼Œè¦æ·±åˆ»å…·ä½“ï¼‰
è¯·æä¾›åˆ›æ–°å¤„çš„ä¸“ä¸šå…·ä½“è¯¦ç»†çš„æ€»ç»“ã€‚

æ‘˜è¦æ€»ç»“:"""
        map_prompt = PromptTemplate.from_template(map_template)
        map_chain = map_prompt | self.llm | StrOutputParser()

        # Reduceé˜¶æ®µæç¤ºè¯
        reduce_template = """ä»¥ä¸‹æ˜¯å¤šç¯‡è®ºæ–‡çš„æ‘˜è¦æ€»ç»“:
{context}
è¯·æ ¹æ®è¿™äº›æ‘˜è¦æ€»ç»“ï¼Œè¿›è¡Œç»¼åˆåˆ†æï¼Œè¦æ±‚ï¼š
1. æŒ‰ä¸»é¢˜æˆ–æ–¹æ³•å¯¹è®ºæ–‡è¿›è¡Œåˆ†ç±»  ï¼ˆè¦æ±‚ï¼šè¯¦ç»†å…·ä½“ï¼Œæœ‰ç»†èŠ‚ï¼Œè¦æ·±åˆ»å…·ä½“ï¼‰
2. è¯†åˆ«ç ”ç©¶ä¸­çš„å…±åŒç‚¹å’Œå·®å¼‚ ï¼ˆè¦æ±‚ï¼šè¯¦ç»†å…·ä½“ï¼Œæœ‰ç»†èŠ‚ï¼Œè¦æ·±åˆ»å…·ä½“ï¼‰
3. æŒ‡å‡ºå…±åŒç ”ç©¶è¶‹åŠ¿ 
4. åˆ†æå½“å‰ç ”ç©¶çš„ç©ºç™½ç‚¹æˆ–æŒ‘æˆ˜

ç»¼åˆåˆ†ææŠ¥å‘Š:"""
        reduce_prompt = PromptTemplate.from_template(reduce_template)
        reduce_chain = reduce_prompt | self.llm | StrOutputParser()

        # è¿”å›mapå’Œreduceé“¾
        return {"map_chain": map_chain, "reduce_chain": reduce_chain}

    def _translate_query_node(self, state: GraphState) -> GraphState:
        """å°†ä¸­æ–‡æŸ¥è¯¢ç¿»è¯‘æˆè‹±æ–‡ï¼Œä»¥ä¾¿æ›´å¥½åœ°åœ¨arXivä¸­æœç´¢"""
        query = state["topic"]
        # å¦‚æœæŸ¥è¯¢å·²ç»æ˜¯è‹±æ–‡ï¼Œç›´æ¥è¿”å›
        if re.match(r'^[a-zA-Z\s]+$', query):
            print("ğŸ“ è¾“å…¥æŸ¥è¯¢è¯å·²ç»æ˜¯è‹±æ–‡ï¼Œæ— éœ€ç¿»è¯‘")
            return {"query": query}

        prompt = f"""
è¯·å°†ä»¥ä¸‹ä¸­æ–‡ç¿»è¯‘æˆé€‚åˆåœ¨å­¦æœ¯æœç´¢å¼•æ“ä¸­ä½¿ç”¨çš„è‹±æ–‡æœ¯è¯­ï¼š
ä¸­æ–‡ï¼š{query}

è¦æ±‚ï¼š
1. ç¿»è¯‘æˆå‡†ç¡®çš„å­¦æœ¯è‹±æ–‡æœ¯è¯­
2. å¦‚æœæœ‰å¤šä¸ªå¯èƒ½çš„ç¿»è¯‘ï¼Œæä¾›æœ€å¸¸è§çš„é‚£ä¸ª
3. åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹
"""

        try:
            print(f"ğŸ¤– æ­£åœ¨è°ƒç”¨LLMè¿›è¡ŒæŸ¥è¯¢è¯ç¿»è¯‘: '{query}'")
            start_time = time.time()
            translation = self.llm.invoke(prompt)
            end_time = time.time()
            translated_query = translation.content.strip()
            print(f"âœ… æŸ¥è¯¢è¯ç¿»è¯‘å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
            print(f"ğŸ” æŸ¥è¯¢è¯ç¿»è¯‘: '{query}' -> '{translated_query}'")
            return {"query": translated_query}
        except Exception as e:
            print(f"âš ï¸ ç¿»è¯‘æŸ¥è¯¢è¯æ—¶å‡ºé”™: {str(e)}, ä½¿ç”¨åŸå§‹æŸ¥è¯¢è¯")
            return {"query": query}

    def _expand_query_node(self, state: GraphState) -> GraphState:
        """æ‰©å±•æŸ¥è¯¢è¯ï¼Œæ·»åŠ ç›¸å…³æœ¯è¯­ä»¥æé«˜æœç´¢æ•ˆæœ"""
        query = state["query"]
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹å­¦æœ¯ä¸»é¢˜æä¾›ç›¸å…³çš„è‹±æ–‡å…³é”®è¯ï¼Œç”¨äºåœ¨arXivä¸­æœç´¢ï¼š
ä¸»é¢˜ï¼š{query}

è¦æ±‚ï¼š
1. æä¾› 5ä¸ªæœ€ç›¸å…³çš„è‹±æ–‡å…³é”®è¯æˆ–çŸ­è¯­
2. åŒ…æ‹¬è¯¥é¢†åŸŸçš„æ ‡å‡†æœ¯è¯­å’Œå¯èƒ½çš„åŒä¹‰è¯
3. ç”¨ORè¿æ¥è¿™äº›å…³é”®è¯ï¼Œå½¢æˆä¸€ä¸ªæœç´¢è¡¨è¾¾å¼
4. åªè¿”å›æœç´¢è¡¨è¾¾å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Š

ä¾‹å¦‚ï¼šmachine learning OR deep learning OR neural networks OR AI
"""

        try:
            print(f"ğŸ¤– æ­£åœ¨è°ƒç”¨LLMè¿›è¡ŒæŸ¥è¯¢è¯æ‰©å±•: '{query}'")
            start_time = time.time()
            expansion = self.llm.invoke(prompt)
            end_time = time.time()
            expanded_query = expansion.content.strip()
            print(f"âœ… æŸ¥è¯¢è¯æ‰©å±•å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
            print(f"ğŸ” æŸ¥è¯¢è¯æ‰©å±•: '{query}' -> '{expanded_query}'")
            return {"query": expanded_query}
        except Exception as e:
            print(f"âš ï¸ æ‰©å±•æŸ¥è¯¢è¯æ—¶å‡ºé”™: {str(e)}, ä½¿ç”¨åŸå§‹æŸ¥è¯¢è¯")
            return {"query": query}

    def _search_papers_node(self, state: GraphState) -> GraphState:
        """æœç´¢arxivè®ºæ–‡"""
        query = state["query"]
        print(f"ğŸ” å¼€å§‹æœç´¢è®ºæ–‡ï¼ŒæŸ¥è¯¢è¯: '{query}'")
        
        try:
            print(f"ğŸ” æ­£åœ¨ä½¿ç”¨æŸ¥è¯¢è¯æœç´¢è®ºæ–‡: '{query}'")
            client = arxiv.Client()

            # è®¡ç®—ä¸¤å¹´å‰çš„æ—¥æœŸ
            two_years_ago = datetime.now().replace(tzinfo=None) - timedelta(days=365*2)

            search = arxiv.Search(
                query=query,
                max_results=max_papers * 2,  # æœç´¢æ›´å¤šè®ºæ–‡ä»¥è¡¥å¿ç­›é€‰
                sort_by=arxiv.SortCriterion.Relevance,
                sort_order=arxiv.SortOrder.Descending
            )

            # æ›´å®‰å…¨åœ°è·å–ç»“æœï¼Œå¤„ç†å¯èƒ½çš„ç©ºé¡µé¢é—®é¢˜
            results = []
            try:
                results = list(client.results(search))
            except Exception as e:
                if "unexpectedly empty" in str(e):
                    print("âš ï¸ æœç´¢ç»“æœé¡µé¢ä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨æ›´ç®€å•çš„æŸ¥è¯¢")
                    # ä½¿ç”¨æ›´ç®€å•çš„æŸ¥è¯¢é‡è¯•
                    simple_query = state["topic"]  # å›é€€åˆ°åŸå§‹ä¸»é¢˜ä½œä¸ºæŸ¥è¯¢è¯
                    print(f"ğŸ”„ ä½¿ç”¨ç®€åŒ–æŸ¥è¯¢é‡è¯•: '{simple_query}'")
                    simple_search = arxiv.Search(
                        query=simple_query,
                        max_results=max_papers,
                        sort_by=arxiv.SortCriterion.Relevance,
                        sort_order=arxiv.SortOrder.Descending
                    )
                    results = list(client.results(simple_search))
                else:
                    raise e

            papers = []

            # ç­›é€‰æœ€è¿‘ä¸¤å¹´çš„è®ºæ–‡
            filtered_results = []
            for result in results:
                published_date = result.published.replace(tzinfo=None)
                if published_date >= two_years_ago:
                    filtered_results.append(result)

            print(f"ğŸ“… ç­›é€‰æœ€è¿‘ä¸¤å¹´è®ºæ–‡: {len(results)} -> {len(filtered_results)}")

            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†è®ºæ–‡
            with ThreadPoolExecutor(max_workers=25) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_index = {
                    executor.submit(self._process_paper_result, result): i
                    for i, result in enumerate(filtered_results[:max_papers])
                }

                # æ”¶é›†ç»“æœ
                for future in tqdm(as_completed(future_to_index),
                                 total=len(future_to_index),
                                 desc="å¤„ç†è®ºæ–‡",
                                 colour="GREEN"):
                    try:
                        paper = future.result(timeout=600)
                        papers.append(paper)
                    except Exception as e:
                        index = future_to_index[future]
                        print(f"âš ï¸ å¤„ç†ç¬¬ {index + 1} ç¯‡è®ºæ–‡æ—¶å‡ºé”™: {e}")

            print(f"âœ… è®ºæ–‡æœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")

            # ä½¿ç”¨è§„åˆ’å™¨è¯„ä¼°è®ºæ–‡ç›¸å…³æ€§å¹¶è°ƒæ•´å…³é”®è¯
            papers = self._planner_filter_papers(papers, state["topic"], max_papers)

            return {"papers": papers}
        except Exception as e:
            print(f"æœç´¢è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
            # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯ä¸­æ–­æ•´ä¸ªæµç¨‹
            return {"papers": []}

    def _process_paper_result(self, result):
        """å¤„ç†å•ä¸ªè®ºæ–‡ç»“æœçš„è¾…åŠ©æ–¹æ³•"""
        print(f"ğŸ“„ è·å–åˆ°è®ºæ–‡: {result.title}")
        paper = {
            "title": result.title,
            "authors": [author.name for author in result.authors],
            "summary": result.summary,
            "published": result.published.strftime("%Y-%m-%d"),
            "pdf_url": result.pdf_url,
            "entry_id": result.entry_id,
            "categories": result.categories
        }

        # å°è¯•æå–PDFå†…å®¹
        if PDF_PROCESSING_AVAILABLE:
            try:
                print(f"ğŸ“„ æ­£åœ¨æå–è®ºæ–‡PDFå†…å®¹: {result.title}")
                paper["full_content"] = self._extract_pdf_content(result.pdf_url)
                print(f"âœ… è®ºæ–‡PDFå†…å®¹æå–å®Œæˆ: {result.title}")
            except Exception as e:
                print(f"âš ï¸ æå–PDFå†…å®¹æ—¶å‡ºé”™: {str(e)}, ä½¿ç”¨æ‘˜è¦å†…å®¹")
                paper["full_content"] = result.summary
        else:
            paper["full_content"] = result.summary
            print(f"ğŸ“„ ä½¿ç”¨è®ºæ–‡æ‘˜è¦å†…å®¹: {result.title}")

        return paper

    def _planner_filter_papers(self, papers: List[Dict], original_query: str, max_results: int) -> List[Dict]:
        """ä½¿ç”¨è§„åˆ’å™¨è¯„ä¼°è®ºæ–‡ç›¸å…³æ€§å¹¶è¿‡æ»¤ä¸ç›¸å…³è®ºæ–‡"""
        if not papers:
            return papers

        print(f"ğŸ§  è§„åˆ’å™¨å¼€å§‹è¯„ä¼° {len(papers)} ç¯‡è®ºæ–‡çš„ç›¸å…³æ€§")

        # è¯„ä¼°æ¯ç¯‡è®ºæ–‡ä¸åŸå§‹æŸ¥è¯¢çš„ç›¸å…³æ€§
        relevant_papers = []
        irrelevant_papers = []

        for paper in papers:
            is_relevant = self._evaluate_paper_relevance(paper, original_query)
            if is_relevant:
                relevant_papers.append(paper)
            else:
                irrelevant_papers.append(paper)

        print(f"âœ… è§„åˆ’å™¨è¯„ä¼°å®Œæˆ: {len(relevant_papers)} ç¯‡ç›¸å…³, {len(irrelevant_papers)} ç¯‡ä¸ç›¸å…³")

        # å¦‚æœç›¸å…³è®ºæ–‡æ•°é‡ä¸è¶³ï¼Œå°è¯•è°ƒæ•´å…³é”®è¯é‡æ–°æœç´¢
        if len(relevant_papers) < max_results * 0.7:  # å¦‚æœç›¸å…³è®ºæ–‡å°‘äº70%
            print(f"âš ï¸ ç›¸å…³è®ºæ–‡ä¸è¶³ï¼Œæ­£åœ¨è°ƒæ•´å…³é”®è¯é‡æ–°æœç´¢...")
            additional_papers = self._search_additional_papers(original_query, max_results - len(relevant_papers), papers)
            relevant_papers.extend(additional_papers)

        # è¿”å›ç›¸å…³è®ºæ–‡ï¼Œé™åˆ¶æ•°é‡
        return relevant_papers[:max_results]

    def _evaluate_paper_relevance(self, paper: Dict, query: str) -> bool:
        """è¯„ä¼°å•ç¯‡è®ºæ–‡ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§"""
        try:
            content = paper.get("full_content", paper["summary"])

            prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹è®ºæ–‡ä¸ç”¨æˆ·æŸ¥è¯¢çš„ç›¸å…³æ€§ï¼š

ç”¨æˆ·æŸ¥è¯¢: {query}

è®ºæ–‡æ ‡é¢˜: {paper['title']}
è®ºæ–‡æ‘˜è¦: {paper['summary'][:1000]}...

è¯·åˆ¤æ–­è¿™ç¯‡è®ºæ–‡æ˜¯å¦ä¸ç”¨æˆ·æŸ¥è¯¢ç›´æ¥ç›¸å…³ã€‚è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
1. è®ºæ–‡ä¸»é¢˜æ˜¯å¦ä¸æŸ¥è¯¢ä¸€è‡´
2. è®ºæ–‡æ˜¯å¦è§£å†³äº†æŸ¥è¯¢ä¸­æåˆ°çš„é—®é¢˜æˆ–ç›¸å…³é—®é¢˜
3. è®ºæ–‡çš„æ–¹æ³•æˆ–æŠ€æœ¯æ˜¯å¦é€‚ç”¨äºæŸ¥è¯¢é¢†åŸŸ

å›ç­”"ç›¸å…³"æˆ–"ä¸ç›¸å…³"ï¼Œå¹¶ç®€è¦è¯´æ˜ç†ç”±ï¼ˆä¸è¶…è¿‡50å­—ï¼‰ã€‚

ä¾‹å¦‚ï¼š
ç›¸å…³ - è®ºæ–‡ä¸»é¢˜ä¸æŸ¥è¯¢å®Œå…¨åŒ¹é…
ä¸ç›¸å…³ - è®ºæ–‡å±äºå®Œå…¨ä¸åŒé¢†åŸŸ
ç›¸å…³ - è™½ç„¶æ–¹æ³•ä¸åŒä½†è§£å†³ç›¸åŒé—®é¢˜
"""

            response = self.llm.invoke(prompt)
            result = response.content.strip().lower()

            # åˆ¤æ–­æ˜¯å¦ç›¸å…³
            is_relevant = "ç›¸å…³" in result or "relevant" in result

            if not is_relevant:
                print(f"ğŸ—‘ï¸  è¿‡æ»¤ä¸ç›¸å…³è®ºæ–‡: {paper['title'][:50]}...")

            return is_relevant
        except Exception as e:
            print(f"âš ï¸ è¯„ä¼°è®ºæ–‡ç›¸å…³æ€§æ—¶å‡ºé”™: {str(e)}, é»˜è®¤è®¤ä¸ºç›¸å…³")
            return True  # å‡ºé”™æ—¶é»˜è®¤è®¤ä¸ºç›¸å…³

    def _search_additional_papers(self, original_query: str, needed_count: int, existing_papers: List[Dict]) -> List[Dict]:
        """æ ¹æ®ç°æœ‰è®ºæ–‡åˆ†æç»“æœï¼Œè°ƒæ•´å…³é”®è¯å¹¶æœç´¢æ›´å¤šç›¸å…³è®ºæ–‡"""
        try:
            # ä»ç°æœ‰è®ºæ–‡ä¸­æå–å…³é”®è¯å’Œä¸»é¢˜
            existing_titles = [paper['title'] for paper in existing_papers]
            existing_summaries = [paper['summary'][:500] for paper in existing_papers]

            prompt = f"""
åŸºäºä»¥ä¸‹å·²æ£€ç´¢åˆ°çš„è®ºæ–‡ï¼Œåˆ†æç”¨æˆ·æŸ¥è¯¢"{original_query}"çš„æ›´ç²¾ç¡®å…³é”®è¯ï¼š

å·²æ£€ç´¢åˆ°çš„è®ºæ–‡æ ‡é¢˜:
{chr(10).join(existing_titles[:5])}

å·²æ£€ç´¢åˆ°çš„è®ºæ–‡æ‘˜è¦:
{chr(10).join(existing_summaries[:3])}

è¯·åˆ†æè¿™äº›è®ºæ–‡çš„å…±åŒç‰¹å¾ï¼Œä¸ºåŸå§‹æŸ¥è¯¢"{original_query}"æä¾›3-5ä¸ªæ›´ç²¾ç¡®çš„è‹±æ–‡å…³é”®è¯æˆ–çŸ­è¯­ï¼Œ
ä»¥ä¾¿æ£€ç´¢åˆ°æ›´ç›¸å…³çš„è®ºæ–‡ã€‚åªéœ€è¦è¿”å›å…³é”®è¯ï¼Œç”¨ORè¿æ¥ï¼Œä¸éœ€è¦è§£é‡Šã€‚

ä¾‹å¦‚ï¼šmachine learning OR deep learning OR neural networks
"""

            response = self.llm.invoke(prompt)
            refined_query = response.content.strip()

            print(f"ğŸ” ä½¿ç”¨è°ƒæ•´åçš„å…³é”®è¯æœç´¢: {refined_query}")

            # ä½¿ç”¨æ–°å…³é”®è¯æœç´¢è®ºæ–‡
            client = arxiv.Client()
            two_years_ago = datetime.now().replace(tzinfo=None) - timedelta(days=365*2)

            search = arxiv.Search(
                query=refined_query,
                max_results=needed_count * 2,
                sort_by=arxiv.SortCriterion.Relevance,
                sort_order=arxiv.SortOrder.Descending
            )

            results = list(client.results(search))

            # ç­›é€‰æœ€è¿‘ä¸¤å¹´çš„è®ºæ–‡
            filtered_results = []
            for result in results:
                published_date = result.published.replace(tzinfo=None)
                if published_date >= two_years_ago:
                    filtered_results.append(result)

            print(f"ğŸ“… ç­›é€‰æœ€è¿‘ä¸¤å¹´è®ºæ–‡: {len(results)} -> {len(filtered_results)}")

            # å¤„ç†æ–°è®ºæ–‡
            additional_papers = []
            for result in filtered_results[:needed_count]:
                try:
                    paper = self._process_paper_result(result)
                    # å†æ¬¡æ£€æŸ¥ç›¸å…³æ€§
                    if self._evaluate_paper_relevance(paper, original_query):
                        additional_papers.append(paper)
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†è¡¥å……è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")

            print(f"âœ… è¡¥å……æœç´¢å®Œæˆï¼Œæ–°å¢ {len(additional_papers)} ç¯‡ç›¸å…³è®ºæ–‡")
            return additional_papers[:needed_count]
        except Exception as e:
            print(f"âš ï¸ è¡¥å……æœç´¢æ—¶å‡ºé”™: {str(e)}")
            return []

    @traceable
    def _extract_pdf_content(self, pdf_url: str) -> str:
        """ä»PDFé“¾æ¥æå–å†…å®¹"""
        try:
            print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½PDFæ–‡ä»¶: {pdf_url}")
            # ä¸‹è½½PDFæ–‡ä»¶
            response = requests.get(pdf_url, timeout=30)
            response.raise_for_status()
            print("âœ… PDFæ–‡ä»¶ä¸‹è½½å®Œæˆ")

            # ä½¿ç”¨PyPDF2è¯»å–PDFå†…å®¹
            print("ğŸ“„ æ­£åœ¨è§£æPDFå†…å®¹")
            pdf_file = io.BytesIO(response.content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)

            # æå–å‰10é¡µçš„å†…å®¹ï¼ˆé€šå¸¸æ˜¯æ‘˜è¦ã€å¼•è¨€å’Œæ–¹æ³•éƒ¨åˆ†ï¼‰
            content = ""
            pages_to_extract = min(20, len(pdf_reader.pages))
            print(f"ğŸ“„ æ­£åœ¨æå–PDFå‰ {pages_to_extract} é¡µå†…å®¹")

            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæå–PDFé¡µé¢å†…å®¹
            page_contents = [""] * pages_to_extract
            with ThreadPoolExecutor(max_workers=25) as executor:
                # æäº¤æ‰€æœ‰é¡µé¢æå–ä»»åŠ¡
                future_to_page = {
                    executor.submit(self._extract_pdf_page, pdf_reader, i): i
                    for i in range(pages_to_extract)
                }

                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_page):
                    try:
                        page_index, page_text = future.result(timeout=120)
                        page_contents[page_index] = page_text
                    except Exception as e:
                        page_index = future_to_page[future]
                        print(f"âš ï¸ æå–ç¬¬ {page_index} é¡µPDFå†…å®¹æ—¶å‡ºé”™: {e}")

            content = "\n".join(page_contents)
            print("âœ… PDFå†…å®¹æå–å®Œæˆ")
            return content
        except Exception as e:
            raise Exception(f"æå–PDFå†…å®¹å¤±è´¥: {str(e)}")

    def _extract_pdf_page(self, pdf_reader, page_index):
        """æå–PDFå•é¡µå†…å®¹çš„è¾…åŠ©æ–¹æ³•"""
        page = pdf_reader.pages[page_index]
        return page_index, page.extract_text() + "\n"

    @traceable
    def _extract_core_content(self, full_content: str) -> str:
        """ä»è®ºæ–‡å®Œæ•´å†…å®¹ä¸­æå–æ ¸å¿ƒåˆ›æ–°æ”¹è¿›æ–¹æ³•å’Œå®éªŒæ•°æ®éƒ¨åˆ†"""
        try:
            # ä½¿ç”¨LLMæå–æ ¸å¿ƒå†…å®¹
            prompt = f"""
è¯·ä»ä»¥ä¸‹è®ºæ–‡å†…å®¹ä¸­æå–æ ¸å¿ƒåˆ›æ–°æ”¹è¿›æ–¹æ³•å’Œå®éªŒæ•°æ®éƒ¨åˆ†ï¼š

è®ºæ–‡å†…å®¹:
{full_content[:80000]}  # é™åˆ¶é•¿åº¦ä»¥é¿å…è¶…å‡ºä¸Šä¸‹æ–‡çª—å£

è¯·æå–ä»¥ä¸‹å†…å®¹ï¼š
1. è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æˆ–æ”¹è¿›æ–¹æ³•
2. å…³é”®çš„å®éªŒæ•°æ®å’Œç»“æœ
3. é‡è¦çš„æ€§èƒ½æŒ‡æ ‡å’Œå¯¹æ¯”ç»“æœ

è¦æ±‚ï¼š
- ä¿æŒåŸæ–‡çš„æŠ€æœ¯ç»†èŠ‚å’Œæ•°æ®å‡†ç¡®æ€§
- åªæå–æœ€å…³é”®å’Œæœ€æœ‰ä»·å€¼çš„ä¿¡æ¯ 
- ä¿æŒç®€æ´ï¼Œæ€»é•¿åº¦ä¸è¶…è¿‡80000å­—
- ä»¥ç»“æ„åŒ–çš„æ–¹å¼å‘ˆç°æå–çš„å†…å®¹

æå–ç»“æœ:
"""

            print("ğŸ¤– æ­£åœ¨è°ƒç”¨LLMæå–è®ºæ–‡æ ¸å¿ƒå†…å®¹")
            start_time = time.time()
            extraction = self.llm.invoke(prompt)
            end_time = time.time()
            print(f"âœ… æ ¸å¿ƒå†…å®¹æå–å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
            return extraction.content.strip()
        except Exception as e:
            print(f"æå–æ ¸å¿ƒå†…å®¹æ—¶å‡ºé”™: {str(e)}, è¿”å›å®Œæ•´å†…å®¹çš„å‰2000ä¸ªå­—ç¬¦")
            return full_content[:2000]

    def _analyze_papers_node(self, state: GraphState) -> GraphState:
        """åˆ†æè®ºæ–‡å¹¶æå–å…³é”®ä¿¡æ¯"""
        papers = state["papers"]
        if not papers:
            return {"analysis": "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡ã€‚"}

        print("ğŸ“Š å¼€å§‹åˆ†æè®ºæ–‡å†…å®¹")
        # ä½¿ç”¨MapReduceæ–¹æ³•åˆ†æè®ºæ–‡
        print("ğŸ”„ æ­¥éª¤1: ä½¿ç”¨MapReduceæ–¹æ³•åˆ†æè®ºæ–‡")
        map_reduce_analysis = self.analyze_papers_with_map_reduce(papers)

        # æ„å»ºè®ºæ–‡ä¿¡æ¯æ€»ç»“
        print("ğŸ”„ æ­¥éª¤2: æ„å»ºè®ºæ–‡è¯¦ç»†ä¿¡æ¯")
        paper_summaries = []

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæå–æ ¸å¿ƒå†…å®¹
        with ThreadPoolExecutor(max_workers=25) as executor:
            # æäº¤æ‰€æœ‰æ ¸å¿ƒå†…å®¹æå–ä»»åŠ¡
            future_to_index = {
                executor.submit(self._extract_summary, paper, i): i
                for i, paper in enumerate(papers)
            }

            # æ”¶é›†ç»“æœ
            summary_results = []
            for future in tqdm(as_completed(future_to_index),
                             total=len(future_to_index),
                             desc="æ„å»ºè®ºæ–‡æ‘˜è¦",
                             colour="MAGENTA"):
                try:
                    index, summary = future.result(timeout=600)
                    summary_results.append((index, summary))
                except Exception as e:
                    index = future_to_index[future]
                    print(f"âš ï¸ æ„å»ºç¬¬ {index + 1} ç¯‡è®ºæ–‡æ‘˜è¦æ—¶å‡ºé”™: {e}")

            # æŒ‰ç´¢å¼•æ’åºä»¥ä¿æŒåŸå§‹é¡ºåº
            summary_results.sort(key=lambda x: x[0])
            paper_summaries = [summary for _, summary in summary_results]

        prompt = f"""
è¯·åˆ†æä»¥ä¸‹{len(papers)}ç¯‡è®ºæ–‡å¹¶æå–å…³é”®ä¿¡æ¯ï¼š

{chr(10).join(paper_summaries)}

è¯·æŒ‰ä»¥ä¸‹è¦æ±‚è¿›è¡Œåˆ†æï¼š
1. æŒ‰ç ”ç©¶ä¸»é¢˜æˆ–æ–¹æ³•å¯¹è®ºæ–‡è¿›è¡Œåˆ†ç±»  ï¼ˆè¦æ±‚ï¼šè¯¦ç»†å…·ä½“ï¼Œæœ‰ç»†èŠ‚ï¼Œè¦æ·±åˆ»å…·ä½“ï¼‰
2. æå–æ¯ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®  ï¼ˆè¦æ±‚ï¼šè¯¦ç»†å…·ä½“ï¼Œæœ‰ç»†èŠ‚ï¼Œè¦æ·±åˆ»å…·ä½“ï¼‰
3. è¯†åˆ«ç ”ç©¶ä¸­çš„å…±åŒç‚¹å’Œå·®å¼‚  ï¼ˆè¦æ±‚ï¼šè¯¦ç»†å…·ä½“ï¼Œæœ‰ç»†èŠ‚ï¼Œè¦æ·±åˆ»å…·ä½“ï¼‰
4. æŒ‡å‡ºç ”ç©¶è¶‹åŠ¿å’Œå‘å±•æ–¹å‘ 
"""

        try:
            print("ğŸ¤– æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œè¯¦ç»†è®ºæ–‡åˆ†æ")
            start_time = time.time()
            analysis = self.llm.invoke(prompt)
            end_time = time.time()
            print(f"âœ… è¯¦ç»†è®ºæ–‡åˆ†æå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
            # ç»“åˆMapReduceåˆ†æç»“æœå’ŒLLMåˆ†æç»“æœ
            combined_analysis = f"""
MapReduceåˆ†æç»“æœ:
{map_reduce_analysis}

è¯¦ç»†åˆ†æç»“æœ:
{analysis.content}
            """
            print("âœ… è®ºæ–‡åˆ†æå®Œæˆ")
            return {"analysis": combined_analysis}
        except Exception as e:
            return {"analysis": f"åˆ†æè®ºæ–‡æ—¶å‡ºé”™: {str(e)}"}

    def analyze_papers_with_map_reduce(self, papers: List[Dict]) -> str:
        """ä½¿ç”¨MapReduceæ–¹æ³•åˆ†æè®ºæ–‡æ‘˜è¦"""
        if not papers:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡ã€‚"

        print(f"ğŸ”„ å¼€å§‹ä½¿ç”¨MapReduceæ–¹æ³•åˆ†æ {len(papers)} ç¯‡è®ºæ–‡")
        # å°†è®ºæ–‡æ‘˜è¦è½¬æ¢ä¸ºDocumentå¯¹è±¡

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæå–æ ¸å¿ƒå†…å®¹
        docs = []
        with ThreadPoolExecutor(max_workers=25) as executor:
            # æäº¤æ‰€æœ‰æ ¸å¿ƒå†…å®¹æå–ä»»åŠ¡
            future_to_index = {
                executor.submit(self._create_document, paper, i, len(papers)): i
                for i, paper in enumerate(papers)
            }

            # æ”¶é›†ç»“æœ
            doc_results = []
            for future in tqdm(as_completed(future_to_index),
                             total=len(future_to_index),
                             desc="æå–è®ºæ–‡æ ¸å¿ƒå†…å®¹",
                             colour="BLUE"):
                try:
                    doc = future.result(timeout=600)
                    doc_results.append(doc)
                except Exception as e:
                    index = future_to_index[future]
                    print(f"âš ï¸ æå–ç¬¬ {index + 1} ç¯‡è®ºæ–‡æ ¸å¿ƒå†…å®¹æ—¶å‡ºé”™: {e}")

            # æŒ‰ç´¢å¼•æ’åºä»¥ä¿æŒåŸå§‹é¡ºåº
            doc_results.sort(key=lambda x: x[0])
            docs = [doc for _, doc in doc_results]

        try:
            # ä½¿ç”¨MapReduceé“¾å¤„ç†æ–‡æ¡£
            map_chain = self.map_reduce_chain["map_chain"]
            reduce_chain = self.map_reduce_chain["reduce_chain"]

            # Mapé˜¶æ®µï¼šå¯¹æ¯ç¯‡è®ºæ–‡è¿›è¡Œåˆ†æ
            print("ğŸ§  å¼€å§‹Mapé˜¶æ®µï¼šé€ç¯‡åˆ†æè®ºæ–‡")
            map_results = []

            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œåˆ†æè®ºæ–‡
            with ThreadPoolExecutor(max_workers=25) as executor:
                # æäº¤æ‰€æœ‰åˆ†æä»»åŠ¡
                future_to_index = {
                    executor.submit(self._analyze_single_paper, map_chain, doc, i): i
                    for i, doc in enumerate(docs)
                }

                # æ”¶é›†ç»“æœ
                analysis_results = []
                for future in tqdm(as_completed(future_to_index),
                                 total=len(future_to_index),
                                 desc="åˆ†æè®ºæ–‡",
                                 colour="YELLOW"):
                    try:
                        index, result = future.result(timeout=600)
                        analysis_results.append((index, result))
                    except Exception as e:
                        index = future_to_index[future]
                        print(f"âš ï¸ åˆ†æç¬¬ {index + 1} ç¯‡è®ºæ–‡æ—¶å‡ºé”™: {e}")

                # æŒ‰ç´¢å¼•æ’åºä»¥ä¿æŒåŸå§‹é¡ºåº
                analysis_results.sort(key=lambda x: x[0])
                map_results = [result for _, result in analysis_results]

            # Reduceé˜¶æ®µï¼šç»¼åˆåˆ†ææ‰€æœ‰ç»“æœ
            print("ğŸ§  å¼€å§‹Reduceé˜¶æ®µï¼šç»¼åˆåˆ†ææ‰€æœ‰è®ºæ–‡")
            combined_context = "\n\n".join([f"è®ºæ–‡åˆ†æ {i+1}:\n{result}" for i, result in enumerate(map_results)])
            print("ğŸ¤– æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œç»¼åˆåˆ†æ")
            start_time = time.time()
            final_result = reduce_chain.invoke({"context": combined_context})
            end_time = time.time()
            print(f"âœ… ç»¼åˆåˆ†æå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")

            print("âœ… MapReduceåˆ†æå®Œæˆ")
            return final_result
        except Exception as e:
            return f"ä½¿ç”¨MapReduceåˆ†æè®ºæ–‡æ—¶å‡ºé”™: {str(e)}"

    def _create_document(self, paper, index, total):
        """åˆ›å»ºæ–‡æ¡£å¯¹è±¡çš„è¾…åŠ©æ–¹æ³•"""
        print(f"ğŸ“„ å¤„ç†è®ºæ–‡ {index+1}/{total}: {paper['title']}")
        core_content = self._extract_core_content(paper.get("full_content", paper["summary"]))

        doc_content = f"""
è®ºæ–‡ {index+1}:
æ ‡é¢˜: {paper['title']}
ä½œè€…: {', '.join(paper['authors'][:3])} ç­‰
å‘è¡¨æ—¥æœŸ: {paper['published']}
åˆ†ç±»: {', '.join(paper['categories'])}
æ ¸å¿ƒå†…å®¹: {core_content}
        """
        return index, Document(page_content=doc_content)

    def _analyze_single_paper(self, map_chain, doc, index):
        """åˆ†æå•ç¯‡è®ºæ–‡çš„è¾…åŠ©æ–¹æ³•"""
        print(f"ğŸ¤– æ­£åœ¨è°ƒç”¨LLMåˆ†æç¬¬ {index+1} ç¯‡è®ºæ–‡")
        start_time = time.time()
        result = map_chain.invoke({"context": doc.page_content})
        end_time = time.time()
        print(f"âœ… ç¬¬ {index+1} ç¯‡è®ºæ–‡åˆ†æå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
        return index, result

    def _extract_summary(self, paper, index):
        """æå–è®ºæ–‡æ‘˜è¦çš„è¾…åŠ©æ–¹æ³•"""
        # æå–æ ¸å¿ƒå†…å®¹æ›¿ä»£æ‘˜è¦
        core_content = self._extract_core_content(paper.get("full_content", paper["summary"]))

        summary = f"""
è®ºæ–‡ {index+1}:
æ ‡é¢˜: {paper['title']}
ä½œè€…: {', '.join(paper['authors'][:3])} ç­‰
å‘è¡¨æ—¥æœŸ: {paper['published']}
åˆ†ç±»: {', '.join(paper['categories'])}
æ ¸å¿ƒå†…å®¹: {core_content[:500]}...
        """
        return index, summary

    def _generate_review_node(self, state: GraphState) -> GraphState:
        """ç”Ÿæˆç»¼è¿°æŠ¥å‘Š"""
        topic = state["topic"]
        papers = state["papers"]
        analysis = state["analysis"]
        
        if not analysis:
            return {"review": "æ²¡æœ‰åˆ†æå†…å®¹å¯ç”¨äºç”Ÿæˆç»¼è¿°ã€‚"}

        print(f"ğŸ“ å¼€å§‹ç”Ÿæˆå…³äº'{topic}'çš„ç»¼è¿°æŠ¥å‘Š")
        # æ„å»ºå‚è€ƒæ–‡çŒ®åˆ—è¡¨
        references = []
        for i, paper in enumerate(papers):
            ref = f"{i+1}. {', '.join(paper['authors'][:3])} et al. {paper['title']}. arXiv:{paper['entry_id'].split('/')[-1]}. {paper['published']}."
            references.append(ref)

        prompt = f"""
åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œç°åœ¨è¯·ç”Ÿæˆä¸€ç¯‡å®Œæ•´çš„å­¦æœ¯ç»¼è¿°æŠ¥å‘Šã€‚

ç ”ç©¶ä¸»é¢˜: {topic}

è®ºæ–‡åˆ†æ:
{analysis}

å‚è€ƒæ–‡çŒ®:
{chr(10).join(references)}

è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç”Ÿæˆç»¼è¿°æŠ¥å‘Šï¼Œä½¿ç”¨Markdownæ ¼å¼ï¼š

# å…³äº{topic}çš„æ–‡çŒ®ç»¼è¿°

## 1. å¼•è¨€
ä»‹ç»ç ”ç©¶é¢†åŸŸå’Œä¸»é¢˜èƒŒæ™¯

## 2. ç ”ç©¶ç°çŠ¶
ä¸“ä¸šæŒ‰ä¸»é¢˜æˆ–æ–¹æ³•åˆ†ç±»è®¨è®ºç›¸å…³ç ”ç©¶ï¼Œå¹¶ç»™å‡ºå…³æ³¨é‡ç‚¹å’Œæ‹“å±•æ–¹å‘

## 3. ä¸»è¦æ–¹æ³•
ä¸“ä¸šæ€»ç»“ç ”ç©¶æ–¹æ³•å’ŒæŠ€æœ¯ï¼Œç»™å‡ºä¸“ä¸šçŠ€åˆ©çš„ç‚¹è¯„å’Œæ ¸å¿ƒæŠ€æœ¯åŸç†

## 4. å®ç”¨æŠ€æœ¯ã€æ¡†æ¶ç»“è®º
æ€»ç»“æ•´ä½“ç ”ç©¶çŠ¶å†µå¹¶æå‡ºå®ç”¨æŠ€æœ¯ã€æ¡†æ¶çš„ä¸“ä¸šè§‚ç‚¹

## å‚è€ƒæ–‡çŒ®
{chr(10).join(references)}

è¦æ±‚ï¼š
- å†…å®¹ä¸“ä¸šã€å‡†ç¡®ã€è¿è´¯
- ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘æ€§å¼º
- å­—æ•°ä¸å°‘äº5000å­—
"""

        try:
            print("ğŸ¤– æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆç»¼è¿°æŠ¥å‘Š")
            start_time = time.time()
            review = self.llm.invoke(prompt)
            end_time = time.time()
            print(f"âœ… ç»¼è¿°æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
            return {"review": review.content}
        except Exception as e:
            return {"review": f"ç”Ÿæˆç»¼è¿°æ—¶å‡ºé”™: {str(e)}"}

    def _save_review_node(self, state: GraphState) -> GraphState:
        """ä¿å­˜ç»¼è¿°æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        review_content = state["review"]
        topic = state["topic"]
        
        if not review_content:
            return {"filename": "æ²¡æœ‰å†…å®¹å¯ä¿å­˜ã€‚"}

        # æ¸…ç†æ–‡ä»¶å
        filename = re.sub(r'[^\w\s-]', '', topic).strip().replace(' ', '_')
        if not filename:
            filename = "literature_review"

        # æ·»åŠ æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename}_{timestamp}.md"

        try:
            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ç»¼è¿°æŠ¥å‘Šåˆ°æ–‡ä»¶: {filename}")
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(review_content)
            print(f"âœ… ç»¼è¿°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
            return {"filename": filename}
        except Exception as e:
            return {"filename": f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"}

    def _create_workflow(self):
        """åˆ›å»ºå·¥ä½œæµå›¾"""
        # å®šä¹‰å›¾
        workflow = StateGraph(GraphState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("translate_query", self._translate_query_node)
        workflow.add_node("expand_query", self._expand_query_node)
        workflow.add_node("search_papers", self._search_papers_node)
        workflow.add_node("analyze_papers", self._analyze_papers_node)
        workflow.add_node("generate_review", self._generate_review_node)
        workflow.add_node("save_review", self._save_review_node)

        # æ·»åŠ è¾¹
        workflow.add_edge(START, "translate_query")
        workflow.add_edge("translate_query", "expand_query")
        workflow.add_edge("expand_query", "search_papers")
        workflow.add_edge("search_papers", "analyze_papers")
        workflow.add_edge("analyze_papers", "generate_review")
        workflow.add_edge("generate_review", "save_review")
        workflow.add_edge("save_review", END)

        return workflow

    def generate_review_for_topic(self, topic: str, max_papers: int = max_papers) -> str:
        """ä¸ºæŒ‡å®šä¸»é¢˜ç”Ÿæˆç»¼è¿°æŠ¥å‘Š"""
        print(f"ğŸ” æ­£åœ¨æœç´¢å…³äº'{topic}'çš„è®ºæ–‡...")
        
        # åˆ›å»ºåˆå§‹çŠ¶æ€
        initial_state = GraphState(
            topic=topic,
            query="",
            papers=[],
            analysis="",
            review="",
            filename="",
            messages=[]
        )
        
        # æ‰§è¡Œå·¥ä½œæµ
        result = self.app.invoke(initial_state)
        
        print(f"âœ… æ‰¾åˆ° {len(result['papers'])} ç¯‡ç›¸å…³è®ºæ–‡")
        if not result['papers']:
            return "æœªèƒ½æ‰¾åˆ°ç›¸å…³è®ºæ–‡ã€‚"

        print("ğŸ“Š è®ºæ–‡åˆ†æå®Œæˆ")
        print("ğŸ“ ç»¼è¿°æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        print("ğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜")
        
        return result["filename"]
        
    def visualize_workflow(self):
        """å¯è§†åŒ–å·¥ä½œæµå›¾"""
        try:
            # ç”Ÿæˆå›¾åƒ
            image_data = self.app.get_graph().draw_mermaid_png()
            # ä¿å­˜å›¾åƒåˆ°æ–‡ä»¶
            with open("workflow_graph.png", "wb") as f:
                f.write(image_data)
            print("âœ… å·¥ä½œæµå›¾å·²ä¿å­˜åˆ° workflow_graph.png")
            
            # åœ¨Jupyter Notebookä¸­æ˜¾ç¤ºå›¾åƒ
            try:
                display(Image("workflow_graph.png"))
            except:
                print("ğŸ’¡ æç¤ºï¼šåœ¨Jupyter Notebookä¸­å¯ä»¥æ˜¾ç¤ºå›¾åƒ")
                
            return "workflow_graph.png"
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå·¥ä½œæµå›¾æ—¶å‡ºé”™: {str(e)}")
            return None


def run_chat_mode():
    """è¿è¡Œå¯¹è¯æ¨¡å¼"""
    print("ğŸ“š åŸºäºarxivçš„è®ºæ–‡æ·±åº¦æœç´¢å’Œç»¼è¿°ç”Ÿæˆç³»ç»Ÿ (å¯¹è¯æ¨¡å¼)")
    print("=" * 60)
    print("æ”¯æŒçš„å‘½ä»¤:")
    print("  - è¾“å…¥ç ”ç©¶ä¸»é¢˜ä»¥ç”Ÿæˆæ–‡çŒ®ç»¼è¿°")
    print("  - 'help' æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
    print("  - 'quit' æˆ– 'exit' é€€å‡ºç³»ç»Ÿ")
    print("=" * 60)

    # åˆå§‹åŒ–LangSmithå®¢æˆ·ç«¯
    client = Client()

    # åˆ›å»ºç»¼è¿°ç”Ÿæˆå™¨
    generator = ArxivReviewGenerator()
    
    # æ˜¾ç¤ºå·¥ä½œæµå›¾
    print("ğŸ”„ æ­£åœ¨ç”Ÿæˆå·¥ä½œæµå›¾...")
    graph_file = generator.visualize_workflow()
    if graph_file:
        print(f"ğŸ“Š å·¥ä½œæµå›¾å·²ç”Ÿæˆ: {graph_file}")

    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nè¯·è¾“å…¥æ‚¨çš„ç ”ç©¶ä¸»é¢˜æˆ–å‘½ä»¤: ").strip()

            # å¤„ç†é€€å‡ºå‘½ä»¤
            if user_input.lower() in ["quit", "exit", "é€€å‡º"]:
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break

            # å¤„ç†å¸®åŠ©å‘½ä»¤
            if user_input.lower() in ["help", "å¸®åŠ©"]:
                print("\nğŸ“– å¸®åŠ©ä¿¡æ¯:")
                print("  1. è¾“å…¥æ‚¨æ„Ÿå…´è¶£çš„ç ”ç©¶ä¸»é¢˜ï¼ˆä¸­æ–‡æˆ–è‹±æ–‡ï¼‰")
                print("  2. ç³»ç»Ÿå°†è‡ªåŠ¨ç¿»è¯‘å¹¶æ‰©å±•æŸ¥è¯¢è¯")
                print("  3. ç³»ç»Ÿä¼šæœç´¢ç›¸å…³è®ºæ–‡å¹¶ç”Ÿæˆç»¼è¿°æŠ¥å‘Š")
                print("  4. æŠ¥å‘Šå°†ä¿å­˜ä¸ºMarkdownæ–‡ä»¶")
                print("\nğŸ“ ç¤ºä¾‹ä¸»é¢˜:")
                print("   - æœºå™¨å­¦ä¹ ")
                print("   - äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨")
                print("   - deep learning")
                continue

            # å¤„ç†ç©ºè¾“å…¥
            if not user_input:
                print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„ç ”ç©¶ä¸»é¢˜æˆ–å‘½ä»¤")
                continue

            # ç”Ÿæˆç»¼è¿°
            print(f"\nğŸš€ å¼€å§‹å¤„ç†ä¸»é¢˜: {user_input}")
            filename = generator.generate_review_for_topic(user_input)
            print(f"\nğŸ‰ æ–‡çŒ®ç»¼è¿°å·²æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜è‡³: {filename}")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œç³»ç»Ÿé€€å‡ºï¼")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
            print("ğŸ”§ è¯·æ£€æŸ¥æ‚¨çš„è¾“å…¥æˆ–ç½‘ç»œè¿æ¥åé‡è¯•")


def main():
    """ä¸»å‡½æ•°"""
    # å¦‚æœæä¾›äº†å‘½ä»¤è¡Œå‚æ•°ï¼Œåˆ™ç›´æ¥å¤„ç†è¯¥ä¸»é¢˜
    import sys
    if len(sys.argv) > 1:
        topic = " ".join(sys.argv[1:])
        generator = ArxivReviewGenerator()
        # æ˜¾ç¤ºå·¥ä½œæµå›¾
        print("ğŸ”„ æ­£åœ¨ç”Ÿæˆå·¥ä½œæµå›¾...")
        graph_file = generator.visualize_workflow()
        if graph_file:
            print(f"ğŸ“Š å·¥ä½œæµå›¾å·²ç”Ÿæˆ: {graph_file}")
            
        filename = generator.generate_review_for_topic(topic)
        print(f"ğŸ‰ æ–‡çŒ®ç»¼è¿°å·²æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜è‡³: {filename}")
        return

    run_chat_mode()


if __name__ == "__main__":
    # è®¾ç½®LangSmithç¯å¢ƒå˜é‡ï¼ˆå¦‚æœå°šæœªè®¾ç½®ï¼‰
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
    # è¯·ç¡®ä¿è®¾ç½®æ‚¨çš„LangSmith APIå¯†é’¥
    os.environ["LANGCHAIN_API_KEY"] = "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"

    # æ£€æŸ¥LangSmithé…ç½®
    langchain_api_key = os.environ.get("LANGCHAIN_API_KEY")
    if not langchain_api_key:
        print("âš ï¸  æ³¨æ„: æœªé…ç½®LANGCHAIN_API_KEYç¯å¢ƒå˜é‡ï¼Œå°†ä¸ä¼šè¿›è¡ŒLangSmithè¿½è¸ª")
        print("   å¦‚éœ€å¯ç”¨è¿½è¸ªï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡:")
        print("   export LANGCHAIN_API_KEY=your-api-key")
        print("   export LANGCHAIN_TRACING_V2=true")
        print("   export LANGCHAIN_ENDPOINT=https://api.smith.langchain.com")
        print("=" * 50)

    main()